{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6431b1f2-90e3-4ca1-8447-22f04f393ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "No GPU found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/3034037371.py:103: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  data = pd.concat([ data, data_dbow.iloc[:,-50:]], 1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#os.chdir(r\"D://Proposal_Defense//Simulations\")\n",
    "#from Utils.Script_utils import get_data_splits, first_LSTM_training, get_data_splits_mod1, get_data_splits_old_algo, get_data_splits_old_algo_doc_word\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count(), flush = True)\n",
    "use_cores  = multiprocessing.cpu_count() - 1\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "model_cbow = gensim.models.word2vec.Word2Vec.load(r\"word2vec_sg\")\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "## Read the data\n",
    "data = pd.read_csv(r\"word2vec_sg.csv\")\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "data_dbow = pd.read_csv(r\"doc2vec_dbow.csv\")\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# np.mean(data_dbow[\"sig_gene_seq\"].values == data[\"sig_gene_seq\"].values)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "data = pd.concat([ data, data_dbow.iloc[:,-50:]], 1)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "data_columns = [['sig_gene_seq'], ['high_level_substr'], list(range(0,100))]\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "data_columns = [inner for item in data_columns for inner in item]\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "data_columns = [str(col) for col in data_columns]\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "data.columns = data_columns\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "## We only using xylan and pectin\n",
    "to_keep = [\"pectin\", \"xylan\"]\n",
    "\n",
    "data = data[data[\"high_level_substr\"].isin(to_keep)]\n",
    "\n",
    "data = data.reset_index(drop = True)\n",
    "\n",
    "features = [seq.replace(\"|\", \",\").replace(\",\", \" \") for seq in data[\"sig_gene_seq\"].values]\n",
    "\n",
    "features  = np.array(features)\n",
    "\n",
    "features = features.reshape(-1,1)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# data[\"high_level_substr\"].value_counts()\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "reps = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579d792a-b004-4c0b-88ed-f9a6dbe1a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done step 1\n",
      "done step 2\n"
     ]
    }
   ],
   "source": [
    "print(\"done step 1\", flush = True)\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "with open('first_lstm_word2vec_doc2vec_var_0.0001.pkl', 'rb') as f:\n",
    "    first_lstm = pickle.load(f)  \n",
    "\n",
    "\n",
    "print(\"done step 2\", flush = True)\n",
    "\n",
    "\n",
    "\n",
    "with open('first_lstm_word2vec_doc2vec_var_0.0001.pkl', 'rb') as f:\n",
    "    first_lstm = pickle.load(f)  \n",
    "\n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_train_probs.pkl', 'rb') as f:\n",
    "    catch_train_probs = pickle.load(f)    \n",
    "\n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_valid_probs.pkl', 'rb') as f:\n",
    "    catch_valid_probs = pickle.load( f)      \n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_test_probs.pkl', 'rb') as f:\n",
    "    catch_test_probs = pickle.load( f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6990772e-4329-456b-b46c-c396fe1bb6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered\n",
      "exited\n"
     ]
    }
   ],
   "source": [
    "print(\"entered\")\n",
    "# var_targets = float(sys.argv[1])\n",
    "var_targets = 0.0001\n",
    "print(\"exited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728c0832-a292-4abe-bd98-4c10fa372747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done step 3\n"
     ]
    }
   ],
   "source": [
    "with open('second_lstm_with_word2vec_doc2vec_var_0.0001.pkl', 'rb') as f:\n",
    "    second_lstm = pickle.load(f)  \n",
    "\n",
    "print(\"done step 3\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e290e5f2-00e5-4109-9efd-c244ce41d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, reps):\n",
    "#     print(np.mean(catch_train_labels_second[i] == catch[i][3].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b06cee0-8d7c-4a24-b43f-d91fc048458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, reps):\n",
    "#     print(np.mean(catch[i][0] == catch_train_seqs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dee29a46-772b-4aee-a244-66b476be87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 10): \n",
    "    input_layer = tf.keras.layers.Input(shape = (X_train_word2vec.shape[1]))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(1)\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "h1, h2 = 16,16\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "X_train_word2vec = data.iloc[:,-50:]\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "ann_15 = ann(h1)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# ann_15.summary()\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "ann_20 = ann(h2)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# ann_20.summary()\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# for i in range(0, reps):\n",
    "#     print(np.abs(tf.math.sigmoid(catch_train_logits[i]).numpy() - tf.math.sigmoid(second_lstm[i][0])).mean())\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "total_weights_1 = ann_15.count_params()\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "total_weights_2 = ann_20.count_params()\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "total_weights = total_weights_1 + total_weights_2 + 1 \n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# total_weights\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "## batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee91a28-1c88-456f-bd5d-7720e09137c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "from scipy.stats import invgamma, norm\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "threshold = 15\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "weights_ann_1 = ann_15.get_weights()\n",
    "weights_ann_2 = ann_20.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a5217b0-0f89-4f01-abf0-87f1346ba706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, batch_data1, initial_ensembles, log_sigma_points_1): \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "\n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "\n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, 1)\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + 1)].reshape(size_ens, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "\n",
    "    n_hidden_2 = len(weights_ann_2[0].ravel())\n",
    "\n",
    "    initial_ensembles_1 = initial_ensembles.copy()[:, total_weights_1:(total_weights_1+ total_weights_2)]\n",
    "\n",
    "    hidden_weights_2 = initial_ensembles_1[:,:n_hidden_2].reshape(size_ens, batch_data.shape[1], h2)\n",
    "\n",
    "\n",
    "\n",
    "    hidden_output_2 = np.einsum('ij,kjl->kil', batch_data1, hidden_weights_2)\n",
    "\n",
    "    hidden_layer_bias_2 = initial_ensembles[:,n_hidden_2:(n_hidden_2 + h2)].reshape(size_ens, 1,  h2)\n",
    "\n",
    "    hidden_output_2 = hidden_output_2+ hidden_layer_bias_2\n",
    "\n",
    "    n_pred_weights_2 = len(weights_ann_2[2].ravel())\n",
    "\n",
    "    output_weights_2 = initial_ensembles_1[:,(n_hidden_2 + h2):(n_hidden_2 + h2 + n_pred_weights_2) ].reshape(size_ens, h2, 1)\n",
    "\n",
    "\n",
    "    output_2 = np.einsum('ijk,ikl->ijl', hidden_output_2, output_weights_2)\n",
    "\n",
    "\n",
    "    output_layer_bias_2 = initial_ensembles_1[:,(n_hidden_2 + h2 + n_pred_weights_2):(n_hidden_2 + h2 + n_pred_weights_2 + 1)].reshape(size_ens, 1, 1)\n",
    "\n",
    "\n",
    "    final_output_2 = output_2 + output_layer_bias_2\n",
    "\n",
    "\n",
    "    weights_1 = initial_ensembles[:, :total_weights_1]\n",
    "\n",
    "    weights_2 = initial_ensembles[:, total_weights_1:(total_weights_1 + total_weights_2)]\n",
    "\n",
    "\n",
    "    avg_weights = initial_ensembles[:, -1].reshape(-1,1)\n",
    "\n",
    "    avg_weights_sig = expit(avg_weights)\n",
    "    \n",
    "    avg_weights_sig = avg_weights_sig.reshape(avg_weights_sig.shape[0], 1, avg_weights_sig.shape[1])\n",
    "    \n",
    "    complement_weights_sig = 1 - expit(avg_weights)\n",
    "    \n",
    "    complement_weights_sig = complement_weights_sig.reshape(complement_weights_sig.shape[0], 1, complement_weights_sig.shape[1])\n",
    "\n",
    "    final_output_1 = final_output_1*complement_weights_sig\n",
    "    \n",
    "    final_output_2 = final_output_2*avg_weights_sig\n",
    "    \n",
    "    output_1_ravel = final_output_1.reshape(size_ens, final_output_1.shape[1]*final_output_1.shape[2])\n",
    "\n",
    "    output_2_ravel = final_output_2.reshape(size_ens, final_output_2.shape[1]*final_output_2.shape[2])\n",
    "\n",
    "\n",
    "    output_1_ravel = output_1_ravel\n",
    "\n",
    "    output_2_ravel = output_2_ravel\n",
    "\n",
    "\n",
    "\n",
    "    weights_1_add = np.zeros((size_ens, (total_weights_2 - total_weights_1)))\n",
    "\n",
    "\n",
    "\n",
    "    weights_1 = np.hstack((weights_1, weights_1_add))\n",
    "    \n",
    "\n",
    "\n",
    "    stack_1 = np.hstack((output_1_ravel, weights_1, np.repeat(0, size_ens).reshape(-1,1), np.repeat(0, size_ens).reshape(-1,1)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    stack_2 = np.hstack((output_2_ravel, weights_2, avg_weights, log_sigma_points_1))\n",
    "\n",
    "    \n",
    "    initial_aug_state = np.hstack((stack_1, stack_2)) \n",
    "    \n",
    "\n",
    "    return initial_aug_state , output_1_ravel, output_2_ravel, log_sigma_points_1\n",
    "\n",
    "\n",
    "# In[88]:\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# In[89]:\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d227ed9e-0a83-4b93-b572-ac2b71b4d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_weights =float(sys.argv[2])\n",
    "var_weights = 16\n",
    "# var_weights_vec = 4\n",
    "# var_targets = 0.04\n",
    "\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "\n",
    "# reduction = float(sys.argv[3])\n",
    "reduction = 8\n",
    "\n",
    "\n",
    "# In[111]:\n",
    "\n",
    "\n",
    "shape_needed = (total_weights + 2*batch_size*1 + 1 + (total_weights_2 - total_weights_1))//reduction\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "size_ens = shape_needed\n",
    "\n",
    "\n",
    "# In[113]:\n",
    "\n",
    "\n",
    "size_ens = int(size_ens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b286e904-3609-4abf-b363-ed64529fde5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea2b6691-a8fd-4323-8acc-234473b281d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "# In[117]:\n",
    "\n",
    "\n",
    "catch1 = second_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191c054d-6b72-4085-b58f-5f282315c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_one(idx, inflation_factor = 0.2, cutoff = 100): \n",
    "    catch_1 = []\n",
    "    catch_2 = []\n",
    "    catch_3 = []\n",
    "    catch_4 = []\n",
    "    catch_5 = []\n",
    "#     from scipy.special import expit\n",
    "    patience_smaller = 0\n",
    "    patience_uns = 0\n",
    "# patience_bigger = 0\n",
    "\n",
    "#     best_train_acc = 0\n",
    "#     best_valid_acc = 1000\n",
    "\n",
    "#     best_valid_mae = 10\n",
    "    \n",
    "    best_train_width = 100\n",
    "    \n",
    "    X_train_logits = catch1[idx][0]\n",
    "    X_valid_logits = catch1[idx][1]\n",
    "    \n",
    "    \n",
    "    X_train_logits = np.vstack((X_train_logits, X_valid_logits))\n",
    "    \n",
    "#     X_train_logits = catch_train_logits_second[idx]\n",
    "\n",
    "    ## create training batch chunks\n",
    "    train_idx = list(range(0, X_train_logits.shape[0]))\n",
    "    batch_chunks = [train_idx[i:i+batch_size] for i in range(0,len(train_idx),batch_size)]\n",
    "\n",
    "    ## generate some augmented variable for iteration 0\n",
    "    initial_aug_state_mean = np.repeat(0, total_weights)\n",
    "    initial_aug_state_mean = initial_aug_state_mean.reshape(-1,1)\n",
    "\n",
    "    initial_aug_state_cov = var_weights*np.identity((total_weights))\n",
    "    initial_ensembles = mvn(initial_aug_state_mean.reshape(initial_aug_state_mean.shape[0],), initial_aug_state_cov).rvs(size = size_ens)\n",
    "\n",
    "    \n",
    "    log_sigma_points_1 = (np.log(gamma(100, scale = 1/100).rvs(size_ens))).reshape(size_ens, 1)\n",
    "    \n",
    "\n",
    "#     y_train = catch_train_labels_second[idx]\n",
    "\n",
    "#     # y_valid = catch_valid_labels_second[idx]\n",
    "\n",
    "#     y_test = catch_test_labels_second[idx]\n",
    "    \n",
    "    \n",
    "    train_word2vec = catch_train_word2vec[idx].values\n",
    "    valid_word2vec = catch_valid_word2vec[idx].values\n",
    "    test_word2vec = catch_test_word2vec[idx].values\n",
    "    \n",
    "    train_word2vec = np.vstack((train_word2vec, valid_word2vec))\n",
    "\n",
    "    best_coverage_train = 0\n",
    "    \n",
    "    threshold_achieved = False    \n",
    "    start=datetime.now()\n",
    "    \n",
    "    for iter1 in range(0,500):\n",
    "\n",
    "        for batch_idx in batch_chunks:\n",
    "\n",
    "            batch_data = train_word2vec[batch_idx,:50]\n",
    "            batch_data1 = train_word2vec[batch_idx,50:]\n",
    "            batch_targets = X_train_logits[batch_idx,:]\n",
    "            batch_targets = batch_targets.ravel().reshape(-1,1)\n",
    "\n",
    "            column_mod_2_shape = total_weights_2 + batch_data.shape[0]*1 + 1 + 1\n",
    "        \n",
    "            H_t = np.hstack((np.identity(batch_targets.shape[0]), np.zeros((batch_targets.shape[0], column_mod_2_shape-batch_targets.shape[0]))))\n",
    "\n",
    "            current_aug_state, column_mod_1, column_mod_2, log_sigma_points_1 = get_targets_with_weights(batch_data, batch_data1, initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "            var_targets_vec = np.log(1 + np.exp(log_sigma_points_1))\n",
    "            \n",
    "            var_targets_vec = var_targets_vec\n",
    "            \n",
    "            # current_aug_state_var = np.cov(current_aug_state.T) + inflation_factor*np.identity(current_aug_state.shape[1])\n",
    "            \n",
    "            current_aug_state_var = np.cov(current_aug_state.T) \n",
    "            \n",
    "            G_t = np.array([1 , 1]).reshape(-1,1)\n",
    "            \n",
    "            scirpt_H_t = np.kron(G_t.T, H_t)\n",
    "\n",
    "            temp1 = current_aug_state_var@scirpt_H_t.T\n",
    "\n",
    "            temp2 = scirpt_H_t@current_aug_state_var@scirpt_H_t.T\n",
    "        \n",
    "            for ensemble_idx in range(0, current_aug_state.shape[0]):\n",
    "                \n",
    "                var_targets1 = var_targets_vec[ensemble_idx,:]\n",
    "                \n",
    "                R_t = var_targets1*np.identity(batch_targets.shape[0])\n",
    "            \n",
    "                measurement_error = mvn(np.repeat(0,batch_targets.shape[0]), var_targets1*np.identity(batch_targets.shape[0])).rvs(1).reshape(-1,1)\n",
    "            \n",
    "                target_current = batch_targets + measurement_error\n",
    "                \n",
    "                K_t = temp1@np.linalg.inv(temp2 + R_t)\n",
    "\n",
    "                current_aug_state[ensemble_idx,:] = current_aug_state[ensemble_idx,:] +(K_t@(target_current -scirpt_H_t@current_aug_state[ensemble_idx,:].reshape(-1,1))).reshape(current_aug_state.shape[1],)\n",
    "        \n",
    "\n",
    "            weights_ann_1 = current_aug_state[:,batch_targets.shape[0]:(batch_targets.shape[0] + total_weights_1)]      \n",
    "\n",
    "            weights_ann_2 = current_aug_state[:,-(total_weights_2+1):-2]    \n",
    "\n",
    "            initial_ensembles = np.hstack((weights_ann_1, weights_ann_2, current_aug_state[:,-2].reshape(-1,1)))\n",
    "            \n",
    "            log_sigma_points_1 = current_aug_state[:,-1].reshape(-1,1)\n",
    "               \n",
    "            avg_betas = expit(current_aug_state[:,-2])\n",
    "        \n",
    "            complement = 1-avg_betas\n",
    "\n",
    "            \n",
    "            current_aug_state1, column_mod_11, column_mod_21, log_sigma_points_1 = get_targets_with_weights(train_word2vec[:, :50], train_word2vec[:, 50:] ,initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "            initial_targets = column_mod_11 + column_mod_21\n",
    "            \n",
    "            \n",
    "            initial_targets = initial_targets.reshape(size_ens, train_word2vec.shape[0],1)\n",
    "            \n",
    "            initial_targets_train = initial_targets\n",
    "            \n",
    "            \n",
    "#             ind = (X_train_logits_true >= np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[0,:,:]) & (X_train_logits_true <= np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "        \n",
    "            initial_targets_softmax = expit(initial_targets)\n",
    "        \n",
    "            initial_softmax_train = initial_targets_softmax\n",
    "            \n",
    "            li = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]\n",
    "            \n",
    "            ui = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "            \n",
    "            width = ui - li\n",
    "            \n",
    "            avg_width_train = np.mean(width)\n",
    "            \n",
    "            temp = np.vstack((catch_train_probs[idx], catch_valid_probs[idx]))\n",
    "            \n",
    "            ind = (temp >= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]) & (temp <= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "            \n",
    "            coverage_train= np.mean(ind.ravel())  \n",
    "            \n",
    "            \n",
    "#             initial_targets_softmax_mean = np.mean(initial_targets_train,0)\n",
    "            \n",
    "#             initial_targets_softmax_std = np.std(initial_targets_train,0)\n",
    "             \n",
    "            # coverage = np.mean(ind.ravel())\n",
    "        \n",
    "            # initial_targets = np.mean(initial_targets,0)\n",
    "            \n",
    "        \n",
    "            # train_mae_logits = np.mean(np.abs(catch_train_logits[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "            # initial_targets = expit(initial_targets)\n",
    "        \n",
    "            # train_mae = np.mean(np.abs(catch_train_probs[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "\n",
    "            # pred_ohe = (initial_targets >= 0.5).astype(float)\n",
    "            # y_train_curr = y_train\n",
    "            # acc = np.mean(pred_ohe == y_train_curr)\n",
    "            \n",
    "#             predicted_batch_1 = column_mod_11\n",
    "#             predicted_batch_2 = column_mod_21\n",
    "            \n",
    "#             predicted_batch_1_ind = predicted_batch_1.reshape(size_ens, train_word2vec.shape[0], 1)\n",
    "            \n",
    "#             predicted_batch_1_ind_train = predicted_batch_1_ind\n",
    "\n",
    "#             predicted_batch_1_ind = np.mean(predicted_batch_1_ind,0)\n",
    "#             predicted_batch_1_ind = expit(predicted_batch_1_ind)\n",
    "        \n",
    "        \n",
    "#             predicted_batch_2_ind = predicted_batch_2.reshape(size_ens, train_word2vec.shape[0], 1)\n",
    "#             predicted_batch_2_ind_train = predicted_batch_2_ind\n",
    "\n",
    "#             predicted_batch_2_ind = np.mean(predicted_batch_2_ind,0)\n",
    "#             predicted_batch_2_ind = expit(predicted_batch_2_ind)\n",
    "        \n",
    "#             predicted_batch_1_ind = (predicted_batch_1_ind >= 0.5).astype(float)\n",
    "#             predicted_batch_2_ind = (predicted_batch_2_ind >= 0.5).astype(float)\n",
    "       \n",
    "#             acc_ind_1_train = np.mean(predicted_batch_1_ind == y_train_curr)\n",
    "#             acc_ind_2_train = np.mean(predicted_batch_2_ind == y_train_curr)\n",
    "        \n",
    "#             acc_ind_1_train_idx =  (predicted_batch_1_ind == y_train_curr).nonzero()\n",
    "#             acc_ind_2_train_idx =  (predicted_batch_2_ind == y_train_curr).nonzero()\n",
    "        \n",
    "#             common_correct = len(set(acc_ind_1_train_idx[0]).intersection(acc_ind_2_train_idx[0]))/len(predicted_batch_1_ind)\n",
    "        \n",
    "        \n",
    "            current_aug_state1, column_mod_11, column_mod_21, log_sigma_points_1 = get_targets_with_weights(test_word2vec[:, :50], test_word2vec[:, 50:] , initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "            initial_targets = column_mod_11 + column_mod_21\n",
    "            \n",
    "            initial_targets = initial_targets.reshape(size_ens, test_word2vec.shape[0],1)\n",
    "            \n",
    "            initial_targets_test = initial_targets\n",
    "            \n",
    "            initial_targets_softmax = expit(initial_targets)    \n",
    "            \n",
    "            li = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]\n",
    "            \n",
    "            ui = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "            \n",
    "            width = ui - li\n",
    "            \n",
    "            avg_width = np.mean(width)\n",
    "            \n",
    "            ind_test = (catch_test_probs[idx] >= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]) & (catch_test_probs[idx] <= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "               \n",
    "#             initial_targets = np.mean(initial_targets,0)\n",
    "\n",
    "#             test_mae_logits = np.mean(np.abs(catch_test_logits[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "            \n",
    "            \n",
    "            # initial_targets_softmax = expit(initial_targets)\n",
    "            \n",
    "            # ind_test = (test_true_probs[idx] >= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]) & (test_true_probs[idx] <= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "                        \n",
    "            coverage_test = np.mean(ind_test.ravel())    \n",
    "            \n",
    "            # test_mae = np.mean(np.abs(catch_test_probs[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "#             pred_ohe = (initial_targets >= 0.5).astype(float)\n",
    "\n",
    "#             y_train_curr = y_test\n",
    "#             acc_test = np.mean(pred_ohe == y_train_curr)        \n",
    "\n",
    "#             enkf_norm = np.linalg.norm(initial_ensembles.mean(0))   \n",
    "            \n",
    "#             catch_1.append(var_targets_vec.mean().round(4))\n",
    "#             catch_2.append(var_targets_vec.std().round(4))\n",
    "#             catch_3.append(np.mean(initial_targets_softmax_std.ravel()**2))\n",
    "#             catch_4.append(np.std(initial_targets_softmax_std.ravel()**2))\n",
    "#             catch_5.append(coverage_train)\n",
    "            \n",
    "#             avg_est_variance = np.mean(var_targets_vec)\n",
    "#             std_est_variance = np.std(var_targets_vec)\n",
    "\n",
    "\n",
    "        if (coverage_train > best_coverage_train) & (coverage_train < 0.95) & (threshold_achieved == False): \n",
    "            cur_best_train_width = avg_width_train\n",
    "            cur_best_test_width = avg_width\n",
    "\n",
    "            cur_best_train_coverage = coverage_train\n",
    "            cur_best_test_coverage = coverage_test \n",
    "            cur_best_lstm_weight = np.mean(complement)\n",
    "            best_coverage_train = coverage_train\n",
    "            exit_iter_no_thresh = iter1\n",
    "            best_test_preds = initial_targets_softmax\n",
    "            patience_uns = 0\n",
    "            threshold_achieved = False\n",
    "            # satisfactory = True\n",
    "            \n",
    "        elif (coverage_train < best_coverage_train) & (coverage_train < 0.95)& (threshold_achieved == False): \n",
    "            patience_uns += 1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            # print(\"something wrong with less than 0.95 case\", flush = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        if (avg_width_train < best_train_width) & (coverage_train >= 0.95):\n",
    "            # print(\"going here\")\n",
    "\n",
    "            best_train_width = avg_width_train\n",
    "            best_test_width = avg_width\n",
    "\n",
    "            best_train_coverage = coverage_train\n",
    "            best_test_coverage = coverage_test\n",
    "            \n",
    "            best_lstm_weight = np.mean(complement)\n",
    "\n",
    "            patience_smaller = 0 \n",
    "            \n",
    "            threshold_achieved = True\n",
    "            exit_iter_thresh = iter1\n",
    "            best_test_preds = initial_targets_softmax\n",
    "            \n",
    "        elif (avg_width_train > best_train_width) & (coverage_train >= 0.95):\n",
    "            patience_smaller +=1\n",
    "            \n",
    "        elif (threshold_achieved == True) & (coverage_train < 0.95):\n",
    "            patience_smaller +=1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            # print(\"something wrong with greater than 0.95 case\", flush = True)\n",
    "            \n",
    "            \n",
    "        # print(\"epoch \"+ str(iter1))\n",
    "        # print(\"patience smaller \"+ str(patience_smaller))\n",
    "        # print(\"patience uns \"+ str(patience_uns))\n",
    "        # # print(\"test mae is \" + str(test_mae))\n",
    "        # print(\"train coverage is \"+ str(coverage_train))\n",
    "        # print(\"train width is \" + str(avg_width_train))        \n",
    "        # print(\"test coverage is \"+ str(coverage_test))\n",
    "        # print(\"test width is \" + str(avg_width))\n",
    "        # print(\"lstm weight is \" + str(np.mean(complement)))\n",
    "        # print(\"threshold \" + str(threshold_achieved))\n",
    "            \n",
    "            \n",
    "        if (threshold_achieved == True) & (coverage_train < 0.95) & (patience_smaller > threshold):\n",
    "            # patience_smaller += 1 \n",
    "            # if patience_smaller > threshold:\n",
    "            print(\"thresh achieved\", flush = True)\n",
    "            stop = datetime.now()\n",
    "            tt = stop-start\n",
    "            mins = tt.seconds/60.0\n",
    "            return best_train_coverage, best_test_coverage, best_train_width, best_test_width, best_lstm_weight, exit_iter_thresh, \"thresh_achieved\", mins ,best_test_preds\n",
    "        \n",
    "        elif (patience_uns > uns_iter_threshold) & (threshold_achieved == False):\n",
    "            print(\"cutting off thresh not achieved\", flush = True)\n",
    "            stop = datetime.now()\n",
    "            tt = stop-start\n",
    "            mins = tt.seconds/60.0            \n",
    "            return cur_best_train_coverage, cur_best_test_coverage, cur_best_train_width, cur_best_test_width, cur_best_lstm_weight, exit_iter_no_thresh,  \"cutoff_thresh_not_achieved\",mins, best_test_preds\n",
    "        \n",
    "        elif (patience_smaller > cutoff_threshold) & (threshold_achieved == True) & (coverage_train > 0.95):\n",
    "            print(\"cutting off thresh achieved\", flush = True)\n",
    "            stop = datetime.now()\n",
    "            tt = stop-start\n",
    "            mins = tt.seconds/60.0              \n",
    "            return best_train_coverage, best_test_coverage, best_train_width, best_test_width, best_lstm_weight, exit_iter_thresh, \"cutoff_thresh_achieved\",mins, best_test_preds\n",
    "        \n",
    "    print(\"something went awry\", flush = True)\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# In[130]:\n",
    "\n",
    "\n",
    "from scipy.stats import gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c756619-eae8-4af0-b761-c7f850383dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_train_labels.pkl', 'rb') as f:\n",
    "    catch_train_labels_second = pickle.load( f)\n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_valid_labels.pkl', 'rb') as f:\n",
    "    catch_valid_labels_second = pickle.load(f)\n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_test_labels.pkl', 'rb') as f:\n",
    "    catch_test_labels_second = pickle.load(f)\n",
    "    \n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_train_word2vec.pkl', 'rb') as f:\n",
    "    catch_train_word2vec = pickle.load(f)\n",
    "    \n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_valid_word2vec.pkl', 'rb') as f:\n",
    "    catch_valid_word2vec = pickle.load(f)\n",
    "    \n",
    "    \n",
    "with open('true_data_word2vec_doc2vec_var_0.0001_test_word2vec.pkl', 'rb') as f:\n",
    "    catch_test_word2vec = pickle.load( f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d640ab8f-e9e4-4e18-bed5-c438bb08efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "073e5119-b99d-4b3a-8f69-384339a450f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "uns_iter_threshold = 30\n",
    "cutoff_threshold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd9d6d5-e17d-4ec9-a999-a8c27e235724",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dda660f-ca95-4e71-858c-996d99fb4f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# best_train_coverage, best_test_coverage, best_train_width, best_test_width, best_lstm_weight, exit_iter_thresh, status, time_taken, best_test_preds = rep_one(cur_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac8b1c1-091a-4616-b05d-bc921ddb37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_train_coverage, best_test_coverage, best_train_width, best_test_width, best_lstm_weight, exit_iter_thresh,time_taken, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a11f913-4e3c-4d53-9b42-8ae9dbd6bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2,4, figsize = (12,6))\n",
    "# axs = axs.ravel()\n",
    "\n",
    "# for i in range(0,8): \n",
    "#     axs[i].hist(np.log(best_test_preds[:,i,:]/(1-best_test_preds[:,i,:])))\n",
    "#     ci = np.percentile(np.log(best_test_preds[:,i,:]/(1-best_test_preds[:,i,:])), q = (2.5, 97.5))\n",
    "#     l, u = ci[0], ci[1]\n",
    "#     axs[i].axvline(x=np.log(catch_test_probs[cur_idx][i]/(1-catch_test_probs[cur_idx][i])), color = \"red\")\n",
    "#     axs[i].axvline(x=l, color = \"green\")\n",
    "#     axs[i].axvline(x=u, color = \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22aff642-bcbd-446e-b254-0103bf531ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2,4, figsize = (12,6))\n",
    "# axs = axs.ravel()\n",
    "\n",
    "# for i in range(0,8): \n",
    "#     axs[i].hist(best_test_preds[:,i,:])\n",
    "#     ci = np.percentile(best_test_preds[:,i,:], q = (2.5, 97.5))\n",
    "#     l, u = ci[0], ci[1]\n",
    "#     axs[i].axvline(x=catch_test_probs[cur_idx][i], color = \"red\")\n",
    "#     axs[i].axvline(x=l, color = \"green\")\n",
    "#     axs[i].axvline(x=u, color = \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88960ba9-030c-4c56-9445-b84d76c7dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_preds = []\n",
    "# for i in range(0, catch_test_probs[cur_idx].shape[0]):\n",
    "#     enkf_preds = best_test_preds[:,i,:]\n",
    "#     enkf_preds_df = pd.DataFrame(enkf_preds)\n",
    "#     enkf_preds_df[\"Test_Sample_ID\"] = i \n",
    "#     enkf_preds_df.columns = [\"EnKF_Preds\", \"Test_Sample_ID\"]\n",
    "#     catch_preds.append(enkf_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "528af0ef-845e-426c-9a36-13ee02ad965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_preds_df = pd.concat(catch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31db638d-48dc-41da-9408-82ca0c4bfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = [i for i in range(0, catch_test_probs[cur_idx].shape[0]) if catch_test_probs[cur_idx][i] < 0.5]\n",
    "# ids_more = [i for i in range(0, catch_test_probs[cur_idx].shape[0]) if catch_test_probs[cur_idx][i] > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30681492-9931-4eb0-9435-01b22cd9da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_more = range(0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3107b27f-6189-404c-85bb-49a9e5f3cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_preds_df_filtered = catch_preds_df[catch_preds_df[\"Test_Sample_ID\"].isin(ids_more)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcdc71e6-6ad9-4cf2-8ba1-ae6dc3ac6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55167b48-9fb3-4b6d-b050-d3084a3985ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_true = catch_test_probs[cur_idx][ids_more,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08601e3f-2344-4bd7-9a8a-767c51072854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.figure(figsize = (10,10))\n",
    "# fig = sns.boxplot(catch_preds_df_filtered, x=\"Test_Sample_ID\", y=\"EnKF_Preds\", showfliers=False)\n",
    "# plt.scatter(range(0, filtered_true.shape[0]), filtered_true, c = \"black\")\n",
    "# # plt.ylim((0.9,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06877184-8c85-4c6d-9926-cdd459557e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:   23.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:   30.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:   48.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed:   58.4s remaining:   49.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed:  1.2min remaining:   36.3s\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n",
      "/tmp/ipykernel_474951/1617587586.py:348: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed:  1.4min remaining:   23.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed:  1.5min remaining:    9.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "catch_coverages = Parallel(n_jobs=15, verbose = 10, backend = \"loky\")(delayed(rep_one)(i) for i in range(reps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e907e33-1b78-4951-b9be-d75ce6a16897",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.DataFrame(catch_coverages).iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f535a03d-7f65-41f4-aacf-9d76ed060458",
   "metadata": {},
   "outputs": [],
   "source": [
    "check.columns = [\"train_coverage\", \"test_coverage\", \"avg_ci_width_train\", \"avg_ci_width_test\", \"avg_word2vec_weight\", \"exit_iter\", \"exit_status\", \"time_taken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3a8d1a9-0849-432e-9c19-7e2c559af023",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = check[[\"exit_status\"]].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd1967fd-6a30-43bf-a628-8f06012caa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.columns = [\"exit_status\", \"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a22b9ce8-805b-4979-80a7-f61eb9e53425",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.to_csv(\"exit_status_EnKF_Word2Vec_Doc2Vec_\" + \"var_weights_\" + str(var_weights) + \"_num_ens_\" + str(size_ens) + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67a55f85-b915-4db0-9afa-450287a5c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474951/3676205243.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  check = check.drop([\"exit_status\"],1)\n"
     ]
    }
   ],
   "source": [
    "check = check.drop([\"exit_status\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5b0112e-2ecc-4fe9-9170-a76ff2b78f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mean = check.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fe950eb-9a47-41f8-b47c-0797e985f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mean.columns = [\"metrics\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0d16718-a1d2-4ec1-8072-2b49c7e5e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mean.to_csv(\"mean_metrics_EnKF_Word2Vec_Doc2Vec_\" + \"var_weights_\" + str(var_weights) + \"_num_ens_\" + str(size_ens) +  \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e614e814-920b-40bc-b17e-cd358a2f3c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_median = check.median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e60508f-fcd4-4da8-ad94-850c96b1bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_median.columns = [\"metrics\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e05396e-c6df-4736-b0bc-9ebcfecf7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_median.to_csv(\"median_metrics_EnKF_Word2Vec_Doc2Vec_\" + \"var_weights_\" + str(var_weights) + \"_num_ens_\" + str(size_ens)+  \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c30a840-9d1e-46e6-ab3f-efc1dc03c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std = check.std().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b698994d-93fe-462e-99f2-193324481319",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std.columns = [\"metrics\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3536d2fc-a072-458b-aee0-a42174eed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std.to_csv(\"std_dev_metrics_EnKF_Word2Vec_Doc2Vec_\" + \"var_weights_\" + str(var_weights) + \"_num_ens_\" + str(size_ens)+  \".csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
