{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6431b1f2-90e3-4ca1-8447-22f04f393ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#os.chdir(r\"D://Proposal_Defense//Simulations\")\n",
    "from Utils.Script_utils import get_data_splits, first_LSTM_training, get_data_splits_mod1, get_data_splits_old_algo, get_data_splits_old_algo_doc_word\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count(), flush = True)\n",
    "use_cores  = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb2e443a-8cb3-4b5a-b112-7035ccdae6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "# from keras import backend as K\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d812f155-8946-437b-aef2-aead8552366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4007900/2246593795.py:33: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  data = pd.concat([ data, data_dbow.iloc[:,-50:]], 1)\n"
     ]
    }
   ],
   "source": [
    "# In[24]:\n",
    "\n",
    "\n",
    "model_cbow = gensim.models.word2vec.Word2Vec.load(r\"word2vec_sg\")\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "## Read the data\n",
    "data = pd.read_csv(r\"word2vec_sg.csv\")\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "data_dbow = pd.read_csv(r\"doc2vec_dbow.csv\")\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# np.mean(data_dbow[\"sig_gene_seq\"].values == data[\"sig_gene_seq\"].values)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "data = pd.concat([ data, data_dbow.iloc[:,-50:]], 1)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "data_columns = [['sig_gene_seq'], ['high_level_substr'], list(range(0,100))]\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "data_columns = [inner for item in data_columns for inner in item]\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "data_columns = [str(col) for col in data_columns]\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "data.columns = data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4be19686-40ac-47b6-b899-f6eaa4fbf949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_gene_seq</th>\n",
       "      <th>high_level_substr</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null,null,HTH_DeoR,HTH_AraC|HTH_AraC,8.A.59,PL...</td>\n",
       "      <td>pectin</td>\n",
       "      <td>0.232673</td>\n",
       "      <td>0.192361</td>\n",
       "      <td>-0.472989</td>\n",
       "      <td>-0.180686</td>\n",
       "      <td>-0.131158</td>\n",
       "      <td>-0.025186</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.140969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169946</td>\n",
       "      <td>-0.143168</td>\n",
       "      <td>0.421863</td>\n",
       "      <td>0.088525</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.218964</td>\n",
       "      <td>0.507033</td>\n",
       "      <td>-0.179407</td>\n",
       "      <td>-0.099657</td>\n",
       "      <td>0.128027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CE10,1.B.35</td>\n",
       "      <td>pectin</td>\n",
       "      <td>0.074477</td>\n",
       "      <td>0.274711</td>\n",
       "      <td>-0.062697</td>\n",
       "      <td>-0.110484</td>\n",
       "      <td>-0.415056</td>\n",
       "      <td>-0.059864</td>\n",
       "      <td>-0.841653</td>\n",
       "      <td>0.507639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042859</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>0.208792</td>\n",
       "      <td>-0.271193</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>0.049114</td>\n",
       "      <td>-0.236957</td>\n",
       "      <td>0.039852</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.016162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.A.1,3.A.1,3.A.1,3.A.1,PL2_2</td>\n",
       "      <td>pectin</td>\n",
       "      <td>0.013718</td>\n",
       "      <td>0.344694</td>\n",
       "      <td>-0.356989</td>\n",
       "      <td>-0.184789</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.124479</td>\n",
       "      <td>-0.196099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059951</td>\n",
       "      <td>-0.108473</td>\n",
       "      <td>0.103364</td>\n",
       "      <td>-0.210818</td>\n",
       "      <td>0.063985</td>\n",
       "      <td>-0.065916</td>\n",
       "      <td>-0.171995</td>\n",
       "      <td>-0.132060</td>\n",
       "      <td>-0.036154</td>\n",
       "      <td>-0.029065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MarR,null,null,AraC_binding,null,null,PfkB,nul...</td>\n",
       "      <td>pectin</td>\n",
       "      <td>0.556954</td>\n",
       "      <td>0.125533</td>\n",
       "      <td>-0.447324</td>\n",
       "      <td>0.080637</td>\n",
       "      <td>-0.138618</td>\n",
       "      <td>-0.212739</td>\n",
       "      <td>-0.059559</td>\n",
       "      <td>0.068977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175302</td>\n",
       "      <td>0.190835</td>\n",
       "      <td>0.102370</td>\n",
       "      <td>-0.651666</td>\n",
       "      <td>-0.101355</td>\n",
       "      <td>-0.007362</td>\n",
       "      <td>-0.107131</td>\n",
       "      <td>-0.218027</td>\n",
       "      <td>0.537375</td>\n",
       "      <td>0.115762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PL10_1,CE8,PL11</td>\n",
       "      <td>pectin</td>\n",
       "      <td>-0.309442</td>\n",
       "      <td>-0.009015</td>\n",
       "      <td>-1.246968</td>\n",
       "      <td>-0.097552</td>\n",
       "      <td>-0.180564</td>\n",
       "      <td>0.267824</td>\n",
       "      <td>0.946469</td>\n",
       "      <td>0.352515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043412</td>\n",
       "      <td>0.048884</td>\n",
       "      <td>0.392662</td>\n",
       "      <td>-0.174932</td>\n",
       "      <td>0.163090</td>\n",
       "      <td>0.252030</td>\n",
       "      <td>0.078837</td>\n",
       "      <td>-0.145878</td>\n",
       "      <td>0.054710</td>\n",
       "      <td>-0.264735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sig_gene_seq high_level_substr  \\\n",
       "0  null,null,HTH_DeoR,HTH_AraC|HTH_AraC,8.A.59,PL...            pectin   \n",
       "1                                        CE10,1.B.35            pectin   \n",
       "2                      3.A.1,3.A.1,3.A.1,3.A.1,PL2_2            pectin   \n",
       "3  MarR,null,null,AraC_binding,null,null,PfkB,nul...            pectin   \n",
       "4                                    PL10_1,CE8,PL11            pectin   \n",
       "\n",
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.232673  0.192361 -0.472989 -0.180686 -0.131158 -0.025186  0.476376   \n",
       "1  0.074477  0.274711 -0.062697 -0.110484 -0.415056 -0.059864 -0.841653   \n",
       "2  0.013718  0.344694 -0.356989 -0.184789  0.378800  0.002130  0.124479   \n",
       "3  0.556954  0.125533 -0.447324  0.080637 -0.138618 -0.212739 -0.059559   \n",
       "4 -0.309442 -0.009015 -1.246968 -0.097552 -0.180564  0.267824  0.946469   \n",
       "\n",
       "          7  ...        90        91        92        93        94        95  \\\n",
       "0  0.140969  ...  0.169946 -0.143168  0.421863  0.088525  0.044629  0.218964   \n",
       "1  0.507639  ... -0.042859  0.027520  0.208792 -0.271193  0.064108  0.049114   \n",
       "2 -0.196099  ... -0.059951 -0.108473  0.103364 -0.210818  0.063985 -0.065916   \n",
       "3  0.068977  ...  0.175302  0.190835  0.102370 -0.651666 -0.101355 -0.007362   \n",
       "4  0.352515  ... -0.043412  0.048884  0.392662 -0.174932  0.163090  0.252030   \n",
       "\n",
       "         96        97        98        99  \n",
       "0  0.507033 -0.179407 -0.099657  0.128027  \n",
       "1 -0.236957  0.039852  0.002365  0.016162  \n",
       "2 -0.171995 -0.132060 -0.036154 -0.029065  \n",
       "3 -0.107131 -0.218027  0.537375  0.115762  \n",
       "4  0.078837 -0.145878  0.054710 -0.264735  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## We only using xylan and pectin\n",
    "to_keep = [\"pectin\", \"xylan\"]\n",
    "\n",
    "data = data[data[\"high_level_substr\"].isin(to_keep)]\n",
    "\n",
    "data = data.reset_index(drop = True)\n",
    "\n",
    "features = [seq.replace(\"|\", \",\").replace(\",\", \" \") for seq in data[\"sig_gene_seq\"].values]\n",
    "\n",
    "features  = np.array(features)\n",
    "\n",
    "features = features.reshape(-1,1)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# data[\"high_level_substr\"].value_counts()\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ce61bdf7-e50d-42dc-bf64-f19d4fa81ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed:    1.0s remaining:    0.8s\n",
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed:    1.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed:    1.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed:    1.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed:    5.4s finished\n"
     ]
    }
   ],
   "source": [
    "# In[35]:\n",
    "\n",
    "\n",
    "reps = 50\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "## generate 100 realizations of train valid and test\n",
    "catch = Parallel(n_jobs=15, verbose = 10, backend = \"loky\")(delayed(get_data_splits_old_algo_doc_word)(data,  features,  i) for i in range(reps)) \n",
    "\n",
    "with open('true_data_word2vec_doc2vec_var_0.0001.pkl', 'wb') as f:\n",
    "    pickle.dump(catch, f)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "615b6458-5ddc-4f5f-ba5e-343f72c0f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention_lstm_model(training): \n",
    "    padding_vector = np.zeros((1, model_cbow.wv.vectors.shape[1]))\n",
    "    unknown_vector = np.zeros((1, model_cbow.wv.vectors.shape[1]))\n",
    "    weight_vectors = np.vstack((padding_vector, unknown_vector))\n",
    "    weight_vectors = np.vstack((weight_vectors, model_cbow.wv.vectors))\n",
    "    embedding_layer = tf.keras.layers.Embedding(len(weight_vectors),\n",
    "                            weight_vectors.shape[1],\n",
    "                            weights=[weight_vectors],\n",
    "                            mask_zero = False,\n",
    "                            trainable=False)\n",
    "\n",
    "    \n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                     output_mode='int',\n",
    "                     vocabulary=model_cbow.wv.index_to_key, \n",
    "                     standardize = None)\n",
    "    \n",
    "    input_layer = tf.keras.layers.Input(shape = (1,), dtype = tf.string)\n",
    "\n",
    "    vectorize = vectorize_layer(input_layer)\n",
    "\n",
    "    vectorize.shape\n",
    "\n",
    "    emb_output = embedding_layer(vectorize)\n",
    "\n",
    "    emb_output.shape\n",
    "    \n",
    "    spatial_drop = tf.keras.layers.SpatialDropout1D(0.5)\n",
    "    \n",
    "    spatial_out = spatial_drop(emb_output, training = training)\n",
    "\n",
    "    lstm_layer = tf.keras.layers.LSTM(25, return_sequences = True, dropout = 0.5)\n",
    "\n",
    "    lstm_output = lstm_layer(spatial_out, training = training)\n",
    "\n",
    "#     x_a = tf.keras.layers.Dense(lstm_output.get_shape()[-1]//2, kernel_initializer = 'glorot_uniform', activation=\"tanh\", name=\"tanh_mlp\")(lstm_output) \n",
    "    \n",
    "    # x_a = tf.keras.layers.SpatialDropout1D(0.25)(lstm_output, training = training)\n",
    "    \n",
    "    x_a = tf.keras.layers.Dense(1, kernel_initializer = 'glorot_uniform', activation='linear', name=\"word-level_context\", kernel_regularizer=tf.keras.regularizers.L2())(lstm_output)\n",
    "\n",
    "    x_a = tf.keras.layers.Flatten()(x_a)\n",
    "\n",
    "    att_out = tf.keras.layers.Activation('softmax')(x_a) \n",
    "\n",
    "    x_a2 = tf.keras.layers.RepeatVector(lstm_output.get_shape()[-1])(att_out)\n",
    "\n",
    "    x_a2 = tf.keras.layers.Permute([2,1])(x_a2)\n",
    "\n",
    "    out = tf.keras.layers.Multiply()([lstm_output,x_a2])\n",
    "    \n",
    "    out = tf.keras.layers.Lambda(lambda x : tf.math.reduce_sum(x, axis = 1), name='expectation_over_words')(out)\n",
    "    \n",
    "    dropout_layer = tf.keras.layers.Dropout(0.65)(out, training = training)\n",
    "\n",
    "    pred_head = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.L2())\n",
    "\n",
    "    pred_output = pred_head(dropout_layer)\n",
    "\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    \n",
    "    model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                 optimizer = tf.keras.optimizers.Adam(learning_rate = lr), \n",
    "                 metrics=tf.keras.metrics.BinaryAccuracy())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "def first_LSTM_training(idx):\n",
    "\n",
    "    first_lstm = attention_lstm_model(False)\n",
    "    # model_word2vec = simple_lstm(False, model_cbow)\n",
    "    init_weights = first_lstm.get_weights()\n",
    "    \n",
    "    first_lstm.fit(catch[idx][0], catch[idx][3], epochs = 2000, verbose = 0, \n",
    "                  callbacks = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 30,\n",
    "                                                              restore_best_weights=True), \n",
    "                      validation_data = (catch[idx][1], catch[idx][4]))\n",
    "    \n",
    "    X_train_logits = first_lstm.predict(catch[idx][0], verbose = 0)\n",
    "    X_valid_logits = first_lstm.predict(catch[idx][1], verbose = 0)\n",
    "    X_test_logits = first_lstm.predict(catch[idx][2], verbose = 0)\n",
    "    \n",
    "    train_acc = first_lstm.evaluate(catch[idx][0], catch[idx][3], verbose = 0)[1]\n",
    "    valid_acc = first_lstm.evaluate(catch[idx][1], catch[idx][4], verbose = 0)[1]\n",
    "    test_acc = first_lstm.evaluate(catch[idx][2], catch[idx][5], verbose = 0)[1]\n",
    "    \n",
    "    return X_train_logits, X_valid_logits, X_test_logits, train_acc,valid_acc, test_acc, init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "478ab600-b7cd-4d49-b176-18ad4fc01413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:   20.0s\n",
      "/home/statgrads/vpiyush2/.conda/envs/enkf/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed:   46.8s remaining:   39.9s\n",
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed:  1.2min remaining:   36.4s\n",
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed:  1.3min remaining:   21.9s\n",
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed:  1.5min remaining:   10.1s\n",
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "first_lstm = Parallel(n_jobs=15, verbose = 10, backend = \"loky\")(delayed(first_LSTM_training)( i) for i in range(reps))\n",
    "with open('first_lstm_word2vec_doc2vec_var_0.0001.pkl', 'wb') as f:\n",
    "    pickle.dump(first_lstm, f)  \n",
    "\n",
    "# print(\"done step 2\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "722a965b-532e-41ac-865f-c8a5d287e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_train_idxes = []\n",
    "catch_train_logits = []\n",
    "catch_train_probs = []\n",
    "catch_train_labels = []\n",
    "catch_train_word2vec = []\n",
    "catch_train_seqs = []\n",
    "# catch_train_word2vec = \n",
    "for i in range(0, reps):\n",
    "    train_true_probs = tf.math.sigmoid(first_lstm[i][0])\n",
    "#     idxes = ((train_true_probs >= 0.05) & (train_true_probs <= 0.95)).numpy().nonzero()[0]\n",
    "#     catch_train_idxes.append(idxes)\n",
    "    train_logits = first_lstm[i][0]\n",
    "    catch_train_logits.append(train_logits)\n",
    "    train_probs = train_true_probs.numpy()\n",
    "    catch_train_probs.append(train_probs)\n",
    "    train_labels = catch[i][3]\n",
    "    catch_train_labels.append(train_labels)\n",
    "    catch_train_word2vec.append(catch[i][6])\n",
    "    catch_train_seqs.append(catch[i][0])\n",
    "\n",
    "\n",
    "\n",
    "catch_valid_idxes = []\n",
    "catch_valid_logits = []\n",
    "catch_valid_probs = []\n",
    "catch_valid_labels = []\n",
    "catch_valid_word2vec = []\n",
    "catch_valid_seqs = []\n",
    "# catch_train_word2vec = \n",
    "for i in range(0, reps):\n",
    "    valid_true_probs = tf.math.sigmoid(first_lstm[i][1])\n",
    "#     idxes = ((train_true_probs >= 0.05) & (train_true_probs <= 0.95)).numpy().nonzero()[0]\n",
    "#     catch_train_idxes.append(idxes)\n",
    "    valid_logits = first_lstm[i][1]\n",
    "    catch_valid_logits.append(valid_logits)\n",
    "    valid_probs = valid_true_probs.numpy()\n",
    "    catch_valid_probs.append(valid_probs)\n",
    "    valid_labels = catch[i][4]\n",
    "    catch_valid_labels.append(valid_labels)\n",
    "    catch_valid_word2vec.append(catch[i][7])\n",
    "    catch_valid_seqs.append(catch[i][1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "catch_test_idxes = []\n",
    "catch_test_logits = []\n",
    "catch_test_probs = []\n",
    "catch_test_labels = []\n",
    "catch_test_word2vec = []\n",
    "catch_test_seqs = []\n",
    "for i in range(0, reps):\n",
    "    test_true_probs = tf.math.sigmoid(first_lstm[i][2])\n",
    "#     idxes = ((test_true_probs >= 0.05) & (test_true_probs <= 0.95)).numpy().nonzero()[0]\n",
    "#     catch_test_idxes.append(idxes)\n",
    "    test_logits = first_lstm[i][2]\n",
    "    catch_test_logits.append(test_logits)\n",
    "    test_probs = test_true_probs.numpy()\n",
    "    catch_test_probs.append(test_probs)\n",
    "    test_labels = catch[i][5]\n",
    "    catch_test_labels.append(test_labels)\n",
    "    catch_test_word2vec.append(catch[i][8])\n",
    "    catch_test_seqs.append(catch[i][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1e1e6574-6bae-492f-8812-29241df4fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_train_word2vec.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_train_word2vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d6b985c8-e875-4d04-bf91-b9fbaa1346ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_valid_word2vec.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_valid_word2vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8f519b08-6533-4220-99d0-2353337bc036",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_test_word2vec.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_test_word2vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8d32c-6863-4e5c-be81-4e9684a84718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac851244-82c2-41e6-b9cf-5d038223dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_train_probs.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_train_probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dc648a62-2e5e-4dc7-b8bc-2bb62938e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_valid_probs.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_valid_probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d433ad37-0e1c-4484-8d3a-dfd0d7f4994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_test_probs.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_test_probs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d792a-b004-4c0b-88ed-f9a6dbe1a4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2aa5f1ae-17af-4a7e-ac9d-3bddef2e3414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered\n",
      "exited\n"
     ]
    }
   ],
   "source": [
    "print(\"entered\")\n",
    "# var_targets = float(sys.argv[1])\n",
    "var_targets = 0.0001\n",
    "print(\"exited\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6990772e-4329-456b-b46c-c396fe1bb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "catch_train_logits_second = []\n",
    "catch_train_probs_second = []\n",
    "catch_train_labels_second = []\n",
    "for i in range(0, reps):\n",
    "    train_true_logits = catch_train_logits[i]\n",
    "    train_logits_noise = norm(0,var_targets).rvs(train_true_logits.shape[0]).reshape(-1,1)\n",
    "    train_logits_second = train_true_logits + train_logits_noise\n",
    "    train_fudged_probs = tf.nn.sigmoid(train_logits_second).numpy()\n",
    "    train_fudged_labels = (train_fudged_probs >= 0.5).astype(float)\n",
    "    # train_fudged_labels = np.random.binomial(1, p=train_fudged_probs)\n",
    "    catch_train_logits_second.append(train_logits_second)\n",
    "    catch_train_probs_second.append(train_fudged_probs)\n",
    "    catch_train_labels_second.append(train_fudged_labels)\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "catch_valid_logits_second = []\n",
    "catch_valid_probs_second = []\n",
    "catch_valid_labels_second = []\n",
    "for i in range(0, reps):\n",
    "    valid_true_logits = catch_valid_logits[i]\n",
    "    valid_logits_noise = norm(0,var_targets).rvs(valid_true_logits.shape[0]).reshape(-1,1)\n",
    "    valid_logits_second = valid_true_logits + valid_logits_noise\n",
    "    valid_fudged_probs = tf.nn.sigmoid(valid_logits_second).numpy()\n",
    "    # valid_fudged_labels = np.random.binomial(1, p=valid_fudged_probs)\n",
    "    valid_fudged_labels = (valid_fudged_probs >= 0.5).astype(float)\n",
    "    catch_valid_logits_second.append(valid_logits_second)\n",
    "    catch_valid_probs_second.append(valid_fudged_probs)\n",
    "    catch_valid_labels_second.append(valid_fudged_labels)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "catch_test_logits_second = []\n",
    "catch_test_probs_second = []\n",
    "catch_test_labels_second = []\n",
    "for i in range(0, reps):\n",
    "    test_true_logits = catch_test_logits[i]\n",
    "    test_logits_noise = norm(0,var_targets).rvs(test_true_logits.shape[0]).reshape(-1,1)\n",
    "    test_logits_second = test_true_logits + test_logits_noise\n",
    "    test_fudged_probs = tf.nn.sigmoid(test_logits_second).numpy()\n",
    "    test_fudged_labels = (test_fudged_probs >= 0.5).astype(float)\n",
    "    # test_fudged_labels = np.random.binomial(1, p=test_fudged_probs)\n",
    "    catch_test_logits_second.append(test_logits_second)\n",
    "    catch_test_probs_second.append(test_fudged_probs)\n",
    "    catch_test_labels_second.append(test_fudged_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dcd5f5ca-6f84-4bf5-8fa4-597b1a984bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_train_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_train_labels_second, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "136ec114-0136-4673-b2f0-3aacba9d2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_valid_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_valid_labels_second, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "65c3f44a-31a2-4153-a235-0376fa3f4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('true_data_word2vec_doc2vec_var_0.0001_test_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(catch_test_labels_second, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e290e5f2-00e5-4109-9efd-c244ce41d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9803921568627451\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9411764705882353\n",
      "0.9607843137254902\n",
      "0.9411764705882353\n",
      "0.9215686274509803\n",
      "0.9215686274509803\n",
      "0.9215686274509803\n",
      "0.9411764705882353\n",
      "0.9411764705882353\n",
      "0.9019607843137255\n",
      "0.9411764705882353\n",
      "0.9019607843137255\n",
      "0.9019607843137255\n",
      "0.9019607843137255\n",
      "0.9215686274509803\n",
      "0.8823529411764706\n",
      "1.0\n",
      "0.9215686274509803\n",
      "0.9607843137254902\n",
      "0.9411764705882353\n",
      "0.9411764705882353\n",
      "0.9411764705882353\n",
      "0.9215686274509803\n",
      "0.9215686274509803\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9607843137254902\n",
      "0.9215686274509803\n",
      "0.8823529411764706\n",
      "0.9411764705882353\n",
      "0.9411764705882353\n",
      "0.9803921568627451\n",
      "0.9215686274509803\n",
      "1.0\n",
      "0.9607843137254902\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, reps):\n",
    "    print(np.mean(catch_train_labels_second[i] == catch[i][3].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5b06cee0-8d7c-4a24-b43f-d91fc048458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, reps):\n",
    "    print(np.mean(catch[i][0] == catch_train_seqs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "010f57f2-aa12-4242-8968-f577060db528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_LSTM_training( idx):\n",
    "#     @tf.function\n",
    "    second_lstm = attention_lstm_model(False)\n",
    "    second_lstm.set_weights(first_lstm[idx][-1])\n",
    "\n",
    "    second_lstm.fit(catch_train_seqs[idx], catch_train_labels_second[idx], epochs = 2000, verbose = 0, \n",
    "                  callbacks = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 30,\n",
    "                                                              restore_best_weights=True), \n",
    "                   validation_data = (catch_valid_seqs[idx], catch_valid_labels_second[idx]))\n",
    "    \n",
    "    second_pred_train_logits = second_lstm.predict(catch_train_seqs[idx], verbose = 0)\n",
    "    second_pred_valid_logits = second_lstm.predict(catch_valid_seqs[idx], verbose = 0)\n",
    "    second_pred_test_logits = second_lstm.predict(catch_test_seqs[idx], verbose = 0)\n",
    "\n",
    "    \n",
    "    return second_pred_train_logits, second_pred_valid_logits, second_pred_test_logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "601a336d-1558-4d71-bab7-c253d83243de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n",
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed:  1.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed:  1.6min remaining:   51.0s\n",
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed:  2.1min remaining:   35.4s\n",
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed:  5.8min remaining:   38.7s\n",
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed:  6.9min finished\n"
     ]
    }
   ],
   "source": [
    "second_lstm = Parallel(n_jobs=15, verbose = 10, backend = \"loky\")(delayed(second_LSTM_training)( i) for i in range(reps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f1ff0517-d5a8-434c-84de-399f7933b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done step 3\n"
     ]
    }
   ],
   "source": [
    "with open('second_lstm_with_word2vec_doc2vec_var_0.0001.pkl', 'wb') as f:\n",
    "    pickle.dump(second_lstm, f)  \n",
    "print(\"done step 3\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fa688-be3f-470e-afcf-7bcb01382e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ann(hidden = 10): \n",
    "#     input_layer = tf.keras.layers.Input(shape = (X_train_word2vec.shape[1]))\n",
    "#     hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "#     hidden_output = hidden_layer(input_layer)\n",
    "#     pred_layer = tf.keras.layers.Dense(1)\n",
    "#     pred_output = pred_layer(hidden_output)\n",
    "# #     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "#     model = tf.keras.models.Model(input_layer, pred_output)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # In[44]:\n",
    "\n",
    "\n",
    "# h1, h2 = 16,16\n",
    "\n",
    "\n",
    "# # In[45]:\n",
    "\n",
    "\n",
    "# X_train_word2vec = data.iloc[:,-50:]\n",
    "\n",
    "\n",
    "# # In[46]:\n",
    "\n",
    "\n",
    "# ann_15 = ann(h1)\n",
    "\n",
    "\n",
    "# # In[14]:\n",
    "\n",
    "\n",
    "# # ann_15.summary()\n",
    "\n",
    "\n",
    "# # In[48]:\n",
    "\n",
    "\n",
    "# ann_20 = ann(h2)\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n",
    "# # ann_20.summary()\n",
    "\n",
    "\n",
    "# # In[16]:\n",
    "\n",
    "\n",
    "# # for i in range(0, reps):\n",
    "# #     print(np.abs(tf.math.sigmoid(catch_train_logits[i]).numpy() - tf.math.sigmoid(second_lstm[i][0])).mean())\n",
    "\n",
    "\n",
    "# # In[51]:\n",
    "\n",
    "\n",
    "# total_weights_1 = ann_15.count_params()\n",
    "\n",
    "\n",
    "# # In[52]:\n",
    "\n",
    "\n",
    "# total_weights_2 = ann_20.count_params()\n",
    "\n",
    "\n",
    "# # In[53]:\n",
    "\n",
    "\n",
    "# total_weights = total_weights_1 + total_weights_2 + 1 \n",
    "\n",
    "\n",
    "# # In[17]:\n",
    "\n",
    "\n",
    "# # total_weights\n",
    "\n",
    "\n",
    "# # In[55]:\n",
    "\n",
    "\n",
    "# ## batch size\n",
    "# batch_size = 32\n",
    "\n",
    "\n",
    "# # In[67]:\n",
    "\n",
    "\n",
    "# # var_weights = 0.08\n",
    "# # var_targets = 0.08\n",
    "\n",
    "\n",
    "# # In[56]:\n",
    "\n",
    "\n",
    "# from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "# # In[57]:\n",
    "\n",
    "\n",
    "# from scipy.stats import invgamma, norm\n",
    "\n",
    "\n",
    "# # In[58]:\n",
    "\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# # In[59]:\n",
    "\n",
    "\n",
    "# threshold = 15\n",
    "\n",
    "\n",
    "# # In[60]:\n",
    "\n",
    "\n",
    "# import random\n",
    "\n",
    "\n",
    "# # In[61]:\n",
    "\n",
    "\n",
    "# lr = 0.1\n",
    "\n",
    "\n",
    "# # In[62]:\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # In[63]:\n",
    "\n",
    "\n",
    "# weights_ann_1 = ann_15.get_weights()\n",
    "# weights_ann_2 = ann_20.get_weights()\n",
    "\n",
    "\n",
    "# # In[87]:\n",
    "\n",
    "\n",
    "# def get_targets_with_weights(batch_data, batch_data1, initial_ensembles, log_sigma_points_1): \n",
    "\n",
    "#     n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "\n",
    "#     hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "\n",
    "    \n",
    "#     hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "#     hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "\n",
    "#     hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "#     n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "#     output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, 1)\n",
    "\n",
    "#     output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "#     output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + 1)].reshape(size_ens, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "#     final_output_1 = output_1 + output_layer_bias_1\n",
    "\n",
    "#     n_hidden_2 = len(weights_ann_2[0].ravel())\n",
    "\n",
    "#     initial_ensembles_1 = initial_ensembles.copy()[:, total_weights_1:(total_weights_1+ total_weights_2)]\n",
    "\n",
    "#     hidden_weights_2 = initial_ensembles_1[:,:n_hidden_2].reshape(size_ens, batch_data.shape[1], h2)\n",
    "\n",
    "\n",
    "\n",
    "#     hidden_output_2 = np.einsum('ij,kjl->kil', batch_data1, hidden_weights_2)\n",
    "\n",
    "#     hidden_layer_bias_2 = initial_ensembles[:,n_hidden_2:(n_hidden_2 + h2)].reshape(size_ens, 1,  h2)\n",
    "\n",
    "#     hidden_output_2 = hidden_output_2+ hidden_layer_bias_2\n",
    "\n",
    "#     n_pred_weights_2 = len(weights_ann_2[2].ravel())\n",
    "\n",
    "#     output_weights_2 = initial_ensembles_1[:,(n_hidden_2 + h2):(n_hidden_2 + h2 + n_pred_weights_2) ].reshape(size_ens, h2, 1)\n",
    "\n",
    "\n",
    "#     output_2 = np.einsum('ijk,ikl->ijl', hidden_output_2, output_weights_2)\n",
    "\n",
    "\n",
    "#     output_layer_bias_2 = initial_ensembles_1[:,(n_hidden_2 + h2 + n_pred_weights_2):(n_hidden_2 + h2 + n_pred_weights_2 + 1)].reshape(size_ens, 1, 1)\n",
    "\n",
    "\n",
    "#     final_output_2 = output_2 + output_layer_bias_2\n",
    "\n",
    "\n",
    "#     weights_1 = initial_ensembles[:, :total_weights_1]\n",
    "\n",
    "#     weights_2 = initial_ensembles[:, total_weights_1:(total_weights_1 + total_weights_2)]\n",
    "\n",
    "\n",
    "#     avg_weights = initial_ensembles[:, -1].reshape(-1,1)\n",
    "\n",
    "#     avg_weights_sig = expit(avg_weights)\n",
    "    \n",
    "#     avg_weights_sig = avg_weights_sig.reshape(avg_weights_sig.shape[0], 1, avg_weights_sig.shape[1])\n",
    "    \n",
    "#     complement_weights_sig = 1 - expit(avg_weights)\n",
    "    \n",
    "#     complement_weights_sig = complement_weights_sig.reshape(complement_weights_sig.shape[0], 1, complement_weights_sig.shape[1])\n",
    "\n",
    "#     final_output_1 = final_output_1*complement_weights_sig\n",
    "    \n",
    "#     final_output_2 = final_output_2*avg_weights_sig\n",
    "    \n",
    "#     output_1_ravel = final_output_1.reshape(size_ens, final_output_1.shape[1]*final_output_1.shape[2])\n",
    "\n",
    "#     output_2_ravel = final_output_2.reshape(size_ens, final_output_2.shape[1]*final_output_2.shape[2])\n",
    "\n",
    "\n",
    "#     output_1_ravel = output_1_ravel\n",
    "\n",
    "#     output_2_ravel = output_2_ravel\n",
    "\n",
    "\n",
    "\n",
    "#     weights_1_add = np.zeros((size_ens, (total_weights_2 - total_weights_1)))\n",
    "\n",
    "\n",
    "\n",
    "#     weights_1 = np.hstack((weights_1, weights_1_add))\n",
    "    \n",
    "\n",
    "\n",
    "#     stack_1 = np.hstack((output_1_ravel, weights_1, np.repeat(0, size_ens).reshape(-1,1), np.repeat(0, size_ens).reshape(-1,1)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     stack_2 = np.hstack((output_2_ravel, weights_2, avg_weights, log_sigma_points_1))\n",
    "\n",
    "    \n",
    "#     initial_aug_state = np.hstack((stack_1, stack_2)) \n",
    "    \n",
    "\n",
    "#     return initial_aug_state , output_1_ravel, output_2_ravel, log_sigma_points_1\n",
    "\n",
    "\n",
    "# # In[88]:\n",
    "\n",
    "\n",
    "# import time\n",
    "\n",
    "\n",
    "# # In[89]:\n",
    "\n",
    "\n",
    "# from scipy.stats import multivariate_normal as mvn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80295acb-e4b3-4127-bbf9-509f4697a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def expit(x):\n",
    "#     \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "# #     e_x = np.exp(x - np.max(x))\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# # In[130]:\n",
    "\n",
    "\n",
    "# from scipy.stats import gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e6a89-08c2-40a0-8703-07eec67c68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rep_one(idx, inflation_factor = 0.2, cutoff = 600): \n",
    "#     catch_1 = []\n",
    "#     catch_2 = []\n",
    "#     catch_3 = []\n",
    "#     catch_4 = []\n",
    "#     catch_5 = []\n",
    "# #     from scipy.special import expit\n",
    "#     patience_smaller = 0\n",
    "# # patience_bigger = 0\n",
    "\n",
    "# #     best_train_acc = 0\n",
    "# #     best_valid_acc = 1000\n",
    "\n",
    "# #     best_valid_mae = 10\n",
    "    \n",
    "#     best_train_width = 100\n",
    "    \n",
    "#     X_train_logits = catch1[idx][0]\n",
    "#     X_valid_logits = catch1[idx][1]\n",
    "    \n",
    "    \n",
    "#     X_train_logits = np.vstack((X_train_logits, X_valid_logits))\n",
    "    \n",
    "# #     X_train_logits = catch_train_logits_second[idx]\n",
    "\n",
    "#     ## create training batch chunks\n",
    "#     train_idx = list(range(0, X_train_logits.shape[0]))\n",
    "#     batch_chunks = [train_idx[i:i+batch_size] for i in range(0,len(train_idx),batch_size)]\n",
    "\n",
    "#     ## generate some augmented variable for iteration 0\n",
    "#     initial_aug_state_mean = np.repeat(0, total_weights)\n",
    "#     initial_aug_state_mean = initial_aug_state_mean.reshape(-1,1)\n",
    "\n",
    "#     initial_aug_state_cov = var_weights*np.identity((total_weights))\n",
    "#     initial_ensembles = mvn(initial_aug_state_mean.reshape(initial_aug_state_mean.shape[0],), initial_aug_state_cov).rvs(size = size_ens)\n",
    "\n",
    "    \n",
    "#     log_sigma_points_1 = (np.log(gamma(20, scale = 1/100).rvs(size_ens))).reshape(size_ens, 1)\n",
    "    \n",
    "\n",
    "# #     y_train = catch_train_labels_second[idx]\n",
    "\n",
    "# #     # y_valid = catch_valid_labels_second[idx]\n",
    "\n",
    "# #     y_test = catch_test_labels_second[idx]\n",
    "    \n",
    "    \n",
    "#     train_word2vec = catch_train_word2vec[idx].values\n",
    "#     valid_word2vec = catch_valid_word2vec[idx].values\n",
    "#     test_word2vec = catch_test_word2vec[idx].values\n",
    "    \n",
    "#     train_word2vec = np.vstack((train_word2vec, valid_word2vec))\n",
    "\n",
    "#     best_coverage_train = 0\n",
    "    \n",
    "#     threshold_achieved = False    \n",
    "    \n",
    "#     for iter1 in range(0,500):\n",
    "\n",
    "#         for batch_idx in batch_chunks:\n",
    "\n",
    "#             batch_data = train_word2vec[batch_idx,:50]\n",
    "#             batch_data1 = train_word2vec[batch_idx,50:]\n",
    "#             batch_targets = X_train_logits[batch_idx,:]\n",
    "#             batch_targets = batch_targets.ravel().reshape(-1,1)\n",
    "\n",
    "#             column_mod_2_shape = total_weights_2 + batch_data.shape[0]*1 + 1 + 1\n",
    "        \n",
    "#             H_t = np.hstack((np.identity(batch_targets.shape[0]), np.zeros((batch_targets.shape[0], column_mod_2_shape-batch_targets.shape[0]))))\n",
    "\n",
    "#             current_aug_state, column_mod_1, column_mod_2, log_sigma_points_1 = get_targets_with_weights(batch_data, batch_data1, initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "#             var_targets_vec = np.log(1 + np.exp(log_sigma_points_1))\n",
    "            \n",
    "#             var_targets_vec = var_targets_vec\n",
    "            \n",
    "#             # current_aug_state_var = np.cov(current_aug_state.T) + inflation_factor*np.identity(current_aug_state.shape[1])\n",
    "            \n",
    "#             current_aug_state_var = np.cov(current_aug_state.T) \n",
    "            \n",
    "#             G_t = np.array([1 , 1]).reshape(-1,1)\n",
    "            \n",
    "#             scirpt_H_t = np.kron(G_t.T, H_t)\n",
    "\n",
    "#             temp1 = current_aug_state_var@scirpt_H_t.T\n",
    "\n",
    "#             temp2 = scirpt_H_t@current_aug_state_var@scirpt_H_t.T\n",
    "        \n",
    "#             for ensemble_idx in range(0, current_aug_state.shape[0]):\n",
    "                \n",
    "#                 var_targets1 = var_targets_vec[ensemble_idx,:]\n",
    "                \n",
    "#                 R_t = var_targets1*np.identity(batch_targets.shape[0])\n",
    "            \n",
    "#                 measurement_error = mvn(np.repeat(0,batch_targets.shape[0]), var_targets1*np.identity(batch_targets.shape[0])).rvs(1).reshape(-1,1)\n",
    "            \n",
    "#                 target_current = batch_targets + measurement_error\n",
    "                \n",
    "#                 K_t = temp1@np.linalg.inv(temp2 + R_t)\n",
    "\n",
    "#                 current_aug_state[ensemble_idx,:] = current_aug_state[ensemble_idx,:] +(K_t@(target_current -scirpt_H_t@current_aug_state[ensemble_idx,:].reshape(-1,1))).reshape(current_aug_state.shape[1],)\n",
    "        \n",
    "\n",
    "#             weights_ann_1 = current_aug_state[:,batch_targets.shape[0]:(batch_targets.shape[0] + total_weights_1)]      \n",
    "\n",
    "#             weights_ann_2 = current_aug_state[:,-(total_weights_2+1):-2]    \n",
    "\n",
    "#             initial_ensembles = np.hstack((weights_ann_1, weights_ann_2, current_aug_state[:,-2].reshape(-1,1)))\n",
    "            \n",
    "#             log_sigma_points_1 = current_aug_state[:,-1].reshape(-1,1)\n",
    "               \n",
    "#             avg_betas = expit(current_aug_state[:,-2])\n",
    "        \n",
    "#             complement = 1-avg_betas\n",
    "\n",
    "            \n",
    "#             current_aug_state1, column_mod_11, column_mod_21, log_sigma_points_1 = get_targets_with_weights(train_word2vec[:, :50], train_word2vec[:, 50:] ,initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "#             initial_targets = column_mod_11 + column_mod_21\n",
    "            \n",
    "            \n",
    "#             initial_targets = initial_targets.reshape(size_ens, train_word2vec.shape[0],1)\n",
    "            \n",
    "#             initial_targets_train = initial_targets\n",
    "            \n",
    "            \n",
    "# #             ind = (X_train_logits_true >= np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[0,:,:]) & (X_train_logits_true <= np.percentile(initial_targets_train, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "        \n",
    "#             initial_targets_softmax = expit(initial_targets)\n",
    "        \n",
    "#             initial_softmax_train = initial_targets_softmax\n",
    "            \n",
    "#             li = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]\n",
    "            \n",
    "#             ui = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "            \n",
    "#             width = ui - li\n",
    "            \n",
    "#             avg_width_train = np.mean(width)\n",
    "            \n",
    "#             temp = np.vstack((catch_train_probs[idx], catch_valid_probs[idx]))\n",
    "            \n",
    "#             ind = (temp >= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]) & (temp <= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "            \n",
    "#             coverage_train= np.mean(ind.ravel())  \n",
    "            \n",
    "            \n",
    "# #             initial_targets_softmax_mean = np.mean(initial_targets_train,0)\n",
    "            \n",
    "# #             initial_targets_softmax_std = np.std(initial_targets_train,0)\n",
    "             \n",
    "#             # coverage = np.mean(ind.ravel())\n",
    "        \n",
    "#             # initial_targets = np.mean(initial_targets,0)\n",
    "            \n",
    "        \n",
    "#             # train_mae_logits = np.mean(np.abs(catch_train_logits[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "#             # initial_targets = expit(initial_targets)\n",
    "        \n",
    "#             # train_mae = np.mean(np.abs(catch_train_probs[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "\n",
    "#             # pred_ohe = (initial_targets >= 0.5).astype(float)\n",
    "#             # y_train_curr = y_train\n",
    "#             # acc = np.mean(pred_ohe == y_train_curr)\n",
    "            \n",
    "# #             predicted_batch_1 = column_mod_11\n",
    "# #             predicted_batch_2 = column_mod_21\n",
    "            \n",
    "# #             predicted_batch_1_ind = predicted_batch_1.reshape(size_ens, train_word2vec.shape[0], 1)\n",
    "            \n",
    "# #             predicted_batch_1_ind_train = predicted_batch_1_ind\n",
    "\n",
    "# #             predicted_batch_1_ind = np.mean(predicted_batch_1_ind,0)\n",
    "# #             predicted_batch_1_ind = expit(predicted_batch_1_ind)\n",
    "        \n",
    "        \n",
    "# #             predicted_batch_2_ind = predicted_batch_2.reshape(size_ens, train_word2vec.shape[0], 1)\n",
    "# #             predicted_batch_2_ind_train = predicted_batch_2_ind\n",
    "\n",
    "# #             predicted_batch_2_ind = np.mean(predicted_batch_2_ind,0)\n",
    "# #             predicted_batch_2_ind = expit(predicted_batch_2_ind)\n",
    "        \n",
    "# #             predicted_batch_1_ind = (predicted_batch_1_ind >= 0.5).astype(float)\n",
    "# #             predicted_batch_2_ind = (predicted_batch_2_ind >= 0.5).astype(float)\n",
    "       \n",
    "# #             acc_ind_1_train = np.mean(predicted_batch_1_ind == y_train_curr)\n",
    "# #             acc_ind_2_train = np.mean(predicted_batch_2_ind == y_train_curr)\n",
    "        \n",
    "# #             acc_ind_1_train_idx =  (predicted_batch_1_ind == y_train_curr).nonzero()\n",
    "# #             acc_ind_2_train_idx =  (predicted_batch_2_ind == y_train_curr).nonzero()\n",
    "        \n",
    "# #             common_correct = len(set(acc_ind_1_train_idx[0]).intersection(acc_ind_2_train_idx[0]))/len(predicted_batch_1_ind)\n",
    "        \n",
    "        \n",
    "#             current_aug_state1, column_mod_11, column_mod_21, log_sigma_points_1 = get_targets_with_weights(test_word2vec[:, :50], test_word2vec[:, 50:] , initial_ensembles, log_sigma_points_1)\n",
    "            \n",
    "#             initial_targets = column_mod_11 + column_mod_21\n",
    "            \n",
    "#             initial_targets = initial_targets.reshape(size_ens, test_word2vec.shape[0],1)\n",
    "            \n",
    "#             initial_targets_test = initial_targets\n",
    "            \n",
    "#             initial_targets_softmax = expit(initial_targets)    \n",
    "            \n",
    "#             li = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]\n",
    "            \n",
    "#             ui = np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:]\n",
    "            \n",
    "#             width = ui - li\n",
    "            \n",
    "#             avg_width = np.mean(width)\n",
    "            \n",
    "#             ind_test = (catch_test_probs[idx] >= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]) & (catch_test_probs[idx] <= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "               \n",
    "# #             initial_targets = np.mean(initial_targets,0)\n",
    "\n",
    "# #             test_mae_logits = np.mean(np.abs(catch_test_logits[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "            \n",
    "            \n",
    "#             # initial_targets_softmax = expit(initial_targets)\n",
    "            \n",
    "#             # ind_test = (test_true_probs[idx] >= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[0,:,:]) & (test_true_probs[idx] <= np.percentile(initial_targets_softmax, axis = 0, q = (2.5, 97.5))[1,:,:])\n",
    "                        \n",
    "#             coverage_test = np.mean(ind_test.ravel())    \n",
    "            \n",
    "#             # test_mae = np.mean(np.abs(catch_test_probs[idx].ravel() - initial_targets.ravel()))\n",
    "        \n",
    "# #             pred_ohe = (initial_targets >= 0.5).astype(float)\n",
    "\n",
    "# #             y_train_curr = y_test\n",
    "# #             acc_test = np.mean(pred_ohe == y_train_curr)        \n",
    "\n",
    "# #             enkf_norm = np.linalg.norm(initial_ensembles.mean(0))   \n",
    "            \n",
    "# #             catch_1.append(var_targets_vec.mean().round(4))\n",
    "# #             catch_2.append(var_targets_vec.std().round(4))\n",
    "# #             catch_3.append(np.mean(initial_targets_softmax_std.ravel()**2))\n",
    "# #             catch_4.append(np.std(initial_targets_softmax_std.ravel()**2))\n",
    "# #             catch_5.append(coverage_train)\n",
    "            \n",
    "# #             avg_est_variance = np.mean(var_targets_vec)\n",
    "# #             std_est_variance = np.std(var_targets_vec)\n",
    "\n",
    "\n",
    "#         if (coverage_train > best_coverage_train) & (coverage_train < 0.95): \n",
    "#             cur_best_train_width = avg_width_train\n",
    "#             cur_best_test_width = avg_width\n",
    "\n",
    "#             cur_best_train_coverage = coverage_train\n",
    "#             cur_best_test_coverage = coverage_test \n",
    "#             # cur_best_lstm_weight = np.mean(complement)\n",
    "#             best_coverage_train = coverage_train\n",
    "#             exit_iter_no_thresh = iter1\n",
    "#             print(cur_best_train_coverage, cur_best_test_coverage, cur_best_train_width, cur_best_test_width, threshold_achieved, iter1)\n",
    "#             # satisfactory = True\n",
    "            \n",
    "        \n",
    "#         if (avg_width_train < best_train_width) & (coverage_train >= 0.95):\n",
    "#             # print(\"going here\")\n",
    "\n",
    "#             best_train_width = avg_width_train\n",
    "#             best_test_width = avg_width\n",
    "\n",
    "#             best_train_coverage = coverage_train\n",
    "#             best_test_coverage = coverage_test\n",
    "            \n",
    "#             # best_lstm_weight = np.mean(complement)\n",
    "\n",
    "#             patience_smaller = 0 \n",
    "            \n",
    "#             threshold_achieved = True\n",
    "#             exit_iter_thresh = iter1\n",
    "            \n",
    "#             print(best_train_coverage, best_test_coverage, best_train_width, best_test_width, threshold_achieved, iter1)\n",
    "            \n",
    "#         if (threshold_achieved == True) & (coverage_train < 0.95):\n",
    "#             patience_smaller += 1 \n",
    "#             print(coverage_train, coverage_test, avg_width_train, avg_width, iter1, patience_smaller)\n",
    "#             if patience_smaller > threshold:\n",
    "#                 print(\"thresh achieved\", flush = True)\n",
    "#                 return best_train_coverage, best_test_coverage, best_train_width, best_test_width, exit_iter_thresh, \"thresh_achieved\"\n",
    "        \n",
    "#         elif (iter1 > iter_threshold) & (threshold_achieved == False):\n",
    "#             print(\"cutting off thresh not achieved\", flush = True)\n",
    "#             return cur_best_train_coverage, cur_best_test_coverage, cur_best_train_width, cur_best_test_width, exit_iter_no_thresh, \"cutoff_thresh_not_achieved\"\n",
    "        \n",
    "#         elif (iter1 > cutoff) & (threshold_achieved == True):\n",
    "#             print(\"cutting off thresh achieved\", flush = True)\n",
    "#             return best_train_coverage, best_test_coverage, best_train_width, best_test_width, exit_iter_thresh, \"cutoff_thresh_achieved\"\n",
    "        \n",
    "#     print(\"something went awry\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b1f61-6f44-4ae8-8294-121174389b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold, iter_threshold = 10, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb8b33-7c69-4d1e-a37b-66de1a0512fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_weights = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096ea43-621a-450a-a4ae-b0f355fd401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8cf47-2b24-4f4d-a425-6b06b8025066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "# # In[110]:\n",
    "\n",
    "\n",
    "# #reduction = 1\n",
    "\n",
    "\n",
    "# # In[111]:\n",
    "\n",
    "\n",
    "# shape_needed = (total_weights + 2*batch_size*1 + 1 + (total_weights_2 - total_weights_1))//reduction\n",
    "\n",
    "\n",
    "# # In[112]:\n",
    "\n",
    "\n",
    "# size_ens = shape_needed\n",
    "\n",
    "\n",
    "# # In[113]:\n",
    "\n",
    "\n",
    "# size_ens = int(size_ens)\n",
    "\n",
    "\n",
    "# # In[18]:\n",
    "\n",
    "\n",
    "# # size_ens\n",
    "\n",
    "\n",
    "# # In[86]:\n",
    "\n",
    "\n",
    "# # train_fudged_probs\n",
    "\n",
    "\n",
    "# # In[115]:\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # In[116]:\n",
    "\n",
    "\n",
    "# from scipy.special import expit\n",
    "\n",
    "\n",
    "# # In[117]:\n",
    "\n",
    "\n",
    "# catch1 = second_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322ffba-1eca-429e-b170-605f86568d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63133b-282d-4632-9383-14d312e57056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_idx =2\n",
    "\n",
    "\n",
    "# best_train_coverage, best_test_coverage, best_train_width, best_test_width, exit_iter_thresh, exit_status  = rep_one(cur_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367beb4-13e1-4a62-b4f8-9f10d413cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_train_coverage, best_test_coverage, best_train_width, best_test_width, exit_iter_thresh, exit_status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
