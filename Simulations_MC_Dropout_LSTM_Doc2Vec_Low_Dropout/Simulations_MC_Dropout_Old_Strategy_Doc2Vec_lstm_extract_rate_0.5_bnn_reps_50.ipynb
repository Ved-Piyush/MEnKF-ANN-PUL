{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ab5bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram is 188\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# os.chdir(r\"D://Proposal_Defense//Simulations\")\n",
    "#from Utils.Script_utils import get_data_splits, first_LSTM_training, get_data_splits_old_algo\n",
    "from joblib import Parallel, delayed\n",
    "import sys\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import psutil\n",
    "    \n",
    "ram_gb = int(np.round(psutil.virtual_memory().total / (1024. **3)))\n",
    "print(\"Ram is \" + str(ram_gb), flush = True)\n",
    "import multiprocessing\n",
    "use_cores = multiprocessing.cpu_count()-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede1177b-8f64-4e6d-adc7-097f20c37da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36084258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e036b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058ca9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow = gensim.models.word2vec.Word2Vec.load(r\"..//Data_Generation//word2vec_sg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72fbb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bbd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440c1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate 100 realizations of train valid and test\n",
    "# catch = Parallel(n_jobs=15, verbose = 10, backend = \"loky\")(delayed(get_data_splits_old_algo)(data,  features,  i) for i in range(reps)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a5cacd-3ebc-4872-9110-8abf4f697b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..//Data_Generation//Data//train_valid_test_splits_50.pkl', 'rb') as f:\n",
    "    catch = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2de221-40c9-4243-af58-d3961efea8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..//Data_Generation//LSTM_Low_Dropout//low_dropout_first_lstm.pkl', 'rb') as f:\n",
    "    first_lstm = pickle.load(f)  \n",
    "\n",
    "    \n",
    "with open('..//Data_Generation//LSTM_Low_Dropout//Low_Data_train_logits.pkl', 'rb') as f:\n",
    "    catch_train_logits = (pickle.load(f))    \n",
    "\n",
    "with open('..//Data_Generation//LSTM_Low_Dropout//Low_Data_valid_logits.pkl', 'rb') as f:\n",
    "    catch_valid_logits = (pickle.load( f))      \n",
    "    \n",
    "with open('..//Data_Generation//LSTM_Low_Dropout//Low_Data_test_logits.pkl', 'rb') as f:\n",
    "    catch_test_logits = (pickle.load( f))      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f0f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..//Data_Generation//LSTM_Low_Dropout//low_dropout_second_lstm.pkl', 'rb') as f:\n",
    "    second_lstm = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533c1d43-8dd2-477d-b9d4-dad3fadf4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, h2 = 16,16\n",
    "\n",
    "## batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64afd6e-9d03-4206-9045-fe622f2ece4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "from scipy.stats import invgamma, norm\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7425ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7959ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2558929a-05a6-494d-9285-58f37943c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch1 = second_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bfe23ff-d473-4d54-9616-703fcf939cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_targets = 0.0001\n",
    "rate = 0.5\n",
    "bnn_reps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a619fc09-b1d2-4000-a54d-4f5e1ed025ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_ensemble_model(idx, dropout_rate = rate, lstm_dim = 25, doc2vec_dim = 50):\n",
    "\n",
    "    # h1, h2 = 16,16\n",
    "    # dimension = 50\n",
    "    # dimension = catch1[idx][3].shape[1]\n",
    "    # input_1 = tf.keras.layers.Input(dimension)\n",
    "    # input_2 = tf.keras.layers.Input(dimension)\n",
    "    input_1 = tf.keras.layers.Input(lstm_dim, name=\"branch_1_input\")\n",
    "    \n",
    "#     input_1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "#     input_1 = input_1_dropout(input_1)\n",
    "    \n",
    "    \n",
    "    input_2 = tf.keras.layers.Input(doc2vec_dim, name=\"branch_2_input\")\n",
    "    \n",
    "#     input_2_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "#     input_2 = input_2_dropout(input_2)\n",
    "    \n",
    "    # train_size = catch1[idx][3].shape[0]\n",
    "    \n",
    "    \n",
    "    hidden_1 = tf.keras.layers.Dense(\n",
    "            h1,\n",
    "       name=\"branch_1_hidden\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    hidden_1_out = hidden_1(input_1)\n",
    "    \n",
    "    hidden_1_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    hidden_1_out = hidden_1_dropout(hidden_1_out, training = True)\n",
    "    \n",
    "    hidden_2  = tf.keras.layers.Dense(\n",
    "            h2,\n",
    "        name=\"branch_2_hidden\"\n",
    "        )\n",
    "    \n",
    "    hidden_2_out = hidden_2(input_2)\n",
    "    \n",
    "    hidden_2_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    hidden_2_out = hidden_2_dropout(hidden_2_out, training = True)    \n",
    "    \n",
    "    \n",
    "    out_1 = tf.keras.layers.Dense(\n",
    "            1,\n",
    "        name=\"branch_1_output\"\n",
    "        )\n",
    "    \n",
    "    out_2 = tf.keras.layers.Dense(\n",
    "            1,\n",
    "        name=\"branch_2_output\"\n",
    "        )\n",
    "    \n",
    "    out_1_out = out_1(hidden_1_out)\n",
    "    out_2_out = out_2(hidden_2_out)\n",
    "    \n",
    "    w2  =  tf.keras.layers.Dense(\n",
    "            1,\n",
    "            activation=\"sigmoid\",\n",
    "        name=\"branch_2_weight\"\n",
    "        )\n",
    "    \n",
    "    w2_out = w2(out_2_out)\n",
    "    \n",
    "#     w1_out = 1 - w2_out\n",
    "    \n",
    "    w1_out = tf.keras.layers.Lambda(lambda x: 1 - x, name = \"branch_1_weight\")(w2_out)\n",
    "    w1_output_1_out = tf.keras.layers.Multiply(name=\"branch_1_weighted_out\")([w1_out, out_1_out])\n",
    "    w2_output_2_out = tf.keras.layers.Multiply(name=\"branch_2_weighted_out\")([w2_out, out_2_out])\n",
    "    \n",
    "    combo_output = tf.keras.layers.Add(name=\"branch_1_2_weighted_avg\")([w1_output_1_out, w2_output_2_out])\n",
    "    bnn_ensemble_model = tf.keras.models.Model([input_1, input_2], combo_output)\n",
    "    \n",
    "    \n",
    "    bnn_ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-1),\n",
    "    loss=\"mse\",\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    return bnn_ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e3ef6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eabfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b982682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_weights =6\n",
    "# var_weights_vec = 4\n",
    "# var_targets = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10d77915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c987219b-dfad-4883-b854-12cb58faa5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebeb0869-7f0b-4f21-be63-0234896af39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fudged_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23d87ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5427e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba10be79-7c26-43e3-a586-72f138d2e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f88c5443-fe08-4fa9-a5df-73d07b2cc0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_dbow = gensim.models.doc2vec.Doc2Vec.load(r\"..//Data_Generation//doc2vec_dbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2378e359-92f7-433c-b98e-53eb2b656101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_one(idx, inflation_factor = 0.2, cutoff = 600): \n",
    "\n",
    "#     from scipy.special import expit\n",
    "    patience_smaller = 0\n",
    "    patience_uns = 0\n",
    "# patience_bigger = 0\n",
    "\n",
    "#     best_train_acc = 0\n",
    "#     best_valid_acc = 1000\n",
    "\n",
    "#     best_valid_mae = 10\n",
    "    \n",
    "    best_train_width = 100\n",
    "    \n",
    "    X_train_logits = np.vstack((catch1[idx][0], catch1[idx][1]))\n",
    "#     X_train_logits = catch_train_logits_second[idx]\n",
    "\n",
    "    ## create training batch chunks\n",
    "    train_idx = list(range(0, X_train_logits.shape[0]))\n",
    "    batch_chunks = [train_idx[i:i+batch_size] for i in range(0,len(train_idx),batch_size)]\n",
    "\n",
    "\n",
    "#     y_train = catch_train_labels_second[idx]\n",
    "\n",
    "#     y_valid = catch_valid_labels_second[idx]\n",
    "\n",
    "#     y_test = catch_test_labels_second[idx]\n",
    "    \n",
    "    \n",
    "    train_lstm = catch1[idx][3].numpy()\n",
    "    valid_lstm = catch1[idx][4].numpy()\n",
    "    # valid_word2vec = catch[idx][7].iloc[catch_valid_idxes[idx],:].reset_index(drop = True)\n",
    "    test_lstm = catch1[idx][5].numpy()\n",
    " \n",
    "\n",
    "    train_doc2vec = []\n",
    "    for seq in catch[idx][0]:\n",
    "        seq_txt = seq[0]\n",
    "        seq_split = seq_txt.split(\" \")\n",
    "        train_doc2vec.append(doc2vec_dbow.infer_vector(seq_split))\n",
    "    train_doc2vec = np.array(train_doc2vec)\n",
    "    \n",
    "    valid_doc2vec = []\n",
    "    for seq in catch[idx][1]:\n",
    "        seq_txt = seq[0]\n",
    "        seq_split = seq_txt.split(\" \")\n",
    "        valid_doc2vec.append(doc2vec_dbow.infer_vector(seq_split))\n",
    "    valid_doc2vec = np.array(valid_doc2vec)\n",
    "    \n",
    "    test_doc2vec = []\n",
    "    for seq in catch[idx][2]:\n",
    "        seq_txt = seq[0]\n",
    "        seq_split = seq_txt.split(\" \")\n",
    "        test_doc2vec.append(doc2vec_dbow.infer_vector(seq_split))\n",
    "    test_doc2vec = np.array(test_doc2vec)    \n",
    "    \n",
    "    train_valid_lstm = np.vstack((train_lstm, valid_lstm))\n",
    "    train_valid_doc2vec = np.vstack((train_doc2vec, valid_doc2vec))\n",
    "\n",
    "    # best_width_train = 100\n",
    "    \n",
    "    threshold_achieved = False\n",
    "    # satisfactory = False\n",
    "    # satisfactory_counter = 0\n",
    "    \n",
    "    ens_model = mc_dropout_ensemble_model(idx)\n",
    "    \n",
    "    best_coverage_train = 0\n",
    "    \n",
    "    start=datetime.now()\n",
    "    \n",
    "    for iter1 in range(0,500):\n",
    "\n",
    "        for batch_idx in batch_chunks:\n",
    "\n",
    "            batch_data = train_valid_lstm[batch_idx,:]\n",
    "            batch_data1 = train_valid_doc2vec[batch_idx,:]\n",
    "            # print(batch_data.shape)\n",
    "            batch_targets = X_train_logits[batch_idx,:]\n",
    "            # batch_targets = batch_targets.ravel().reshape(-1,1)\n",
    "            ens_model.fit([batch_data, batch_data1],batch_targets, epochs=1, verbose = 0)\n",
    "        \n",
    "            all_preds1 = [np.array(ens_model([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            all_preds1 = np.hstack(all_preds1)\n",
    "\n",
    "            all_preds2 = expit(all_preds1)\n",
    "            all_preds3 = all_preds2.mean(1).reshape(-1,1)\n",
    "\n",
    "            li = np.percentile(all_preds2, axis = 1, q = (2.5, 97.5))[0,:].reshape(-1,1)\n",
    "            \n",
    "            ui = np.percentile(all_preds2, axis = 1, q = (2.5, 97.5))[1,:].reshape(-1,1)\n",
    "            \n",
    "            width = ui - li\n",
    "            \n",
    "            avg_width_train = np.mean(width)\n",
    "            \n",
    "            temp = expit(np.vstack((catch_train_logits[idx], catch_valid_logits[idx])))\n",
    "            \n",
    "            ind = (temp >= li) & (temp <= ui)\n",
    "            \n",
    "            coverage_train= np.mean(ind.ravel())  \n",
    "            \n",
    "            \n",
    "            all_preds1 = [np.array(ens_model([test_lstm, test_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            all_preds1 = np.hstack(all_preds1)\n",
    "            all_preds2 = expit(all_preds1)\n",
    "            all_preds3 = all_preds2.mean(1).reshape(-1,1)\n",
    "            \n",
    "            li = np.percentile(all_preds2, axis = 1, q = (2.5, 97.5))[0,:].reshape(-1,1)\n",
    "            \n",
    "            ui = np.percentile(all_preds2, axis = 1, q = (2.5, 97.5))[1,:].reshape(-1,1)\n",
    "            \n",
    "            width = ui - li\n",
    "            \n",
    "            avg_width = np.mean(width)\n",
    "            \n",
    "            ind_test = (expit(catch_test_logits[idx]) >= li) & (expit(catch_test_logits[idx]) <= ui)\n",
    "               \n",
    " \n",
    "            coverage_test = np.mean(ind_test.ravel())    \n",
    "              \n",
    "    \n",
    "\n",
    "    \n",
    "        if (coverage_train > best_coverage_train) & (coverage_train < 0.95) & (threshold_achieved == False): \n",
    "            cur_best_train_width = avg_width_train\n",
    "            cur_best_test_width = avg_width\n",
    "\n",
    "            cur_best_train_coverage = coverage_train\n",
    "            cur_best_test_coverage = coverage_test \n",
    "            # cur_best_lstm_weight = np.mean(complement)\n",
    "            best_coverage_train = coverage_train\n",
    "            exit_iter_no_thresh = iter1\n",
    "            # best_test_preds = initial_targets_softmax\n",
    "            patience_uns = 0\n",
    "            threshold_achieved = False         \n",
    "            # satisfactory = True\n",
    "            \n",
    "        elif (coverage_train < best_coverage_train) & (coverage_train < 0.95)& (threshold_achieved == False): \n",
    "            patience_uns += 1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            # print(\"something wrong with less than 0.95 case\", flush = True)\n",
    "        \n",
    "        if (avg_width_train < best_train_width) & (coverage_train >= 0.95):\n",
    "            # print(\"going here\")\n",
    "\n",
    "            best_train_width = avg_width_train\n",
    "            best_test_width = avg_width\n",
    "\n",
    "            best_train_coverage = coverage_train\n",
    "            best_test_coverage = coverage_test\n",
    "            \n",
    "            # best_lstm_weight = np.mean(complement)\n",
    "\n",
    "            patience_smaller = 0 \n",
    "            \n",
    "            threshold_achieved = True\n",
    "            exit_iter_thresh = iter1\n",
    "            # best_test_preds = initial_targets_softmax\n",
    "            \n",
    "            \n",
    "        elif (avg_width_train > best_train_width) & (coverage_train >= 0.95):\n",
    "            patience_smaller +=1\n",
    "            \n",
    "        elif (threshold_achieved == True) & (coverage_train < 0.95):\n",
    "            patience_smaller +=1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            # print(\"something wrong with greater than 0.95 case\", flush = True)\n",
    "            \n",
    "            \n",
    "        # print(\"epoch \"+ str(iter1))\n",
    "        # print(\"patience smaller \"+ str(patience_smaller))\n",
    "        # print(\"patience uns \"+ str(patience_uns))\n",
    "        # # print(\"test mae is \" + str(test_mae))\n",
    "        # print(\"train coverage is \"+ str(coverage_train))\n",
    "        # print(\"train width is \" + str(avg_width_train))        \n",
    "        # print(\"test coverage is \"+ str(coverage_test))\n",
    "        # print(\"test width is \" + str(avg_width))\n",
    "        # # print(\"lstm weight is \" + str(np.mean(complement)))\n",
    "        # print(\"threshold \" + str(threshold_achieved))\n",
    "            \n",
    "            \n",
    "        if (threshold_achieved == True) & (coverage_train < 0.95) & (patience_smaller > threshold):\n",
    "            lstm_weight_extract_mod = tf.keras.models.Model(ens_model.input, ens_model.layers[-5].output)\n",
    "            lstm_weight_array = [np.array(lstm_weight_extract_mod([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            lstm_weight_array = np.hstack(lstm_weight_array)\n",
    "            best_avg_lstm_weight = lstm_weight_array.mean(0).mean()\n",
    "            # patience_smaller += 1 \n",
    "            # if patience_smaller > threshold:\n",
    "            print(\"thresh achieved\", flush = True)\n",
    "            # lstm_weight_extract_mod = tf.keras.models.Model(ens_model.input, ens_model.layers[-5].output)\n",
    "            # lstm_weight_array = [np.array(lstm_weight_extract_mod([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            # lstm_weight_array = np.hstack(lstm_weight_array)\n",
    "            # cur_best_lstm_weight = lstm_weight_array.mean(0).mean()\n",
    "            stop = datetime.now()\n",
    "            tt = stop-start\n",
    "            mins = tt.seconds/60.0            \n",
    "            return best_train_coverage, best_test_coverage, best_train_width, best_test_width, best_avg_lstm_weight,  exit_iter_thresh, \"thresh_achieved\", mins \n",
    "        \n",
    "        elif (patience_uns > uns_iter_threshold) & (threshold_achieved == False):\n",
    "            lstm_weight_extract_mod = tf.keras.models.Model(ens_model.input, ens_model.layers[-5].output)\n",
    "            lstm_weight_array = [np.array(lstm_weight_extract_mod([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            lstm_weight_array = np.hstack(lstm_weight_array)\n",
    "            best_avg_lstm_weight = lstm_weight_array.mean(0).mean()            \n",
    "            print(\"cutting off thresh not achieved\", flush = True)\n",
    "            # lstm_weight_extract_mod = tf.keras.models.Model(ens_model.input, ens_model.layers[-5].output)\n",
    "            # lstm_weight_array = [np.array(lstm_weight_extract_mod([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            # lstm_weight_array = np.hstack(lstm_weight_array)\n",
    "            # cur_best_lstm_weight = lstm_weight_array.mean(0).mean()    \n",
    "            stop = datetime.now()\n",
    "            tt = stop-start\n",
    "            mins = tt.seconds/60.0            \n",
    "            return cur_best_train_coverage, cur_best_test_coverage, cur_best_train_width, cur_best_test_width, best_avg_lstm_weight,  exit_iter_no_thresh,  \"cutoff_thresh_not_achieved\",mins\n",
    "        \n",
    "        elif (patience_smaller > cutoff_threshold) & (threshold_achieved == True) & (coverage_train > 0.95):\n",
    "            lstm_weight_extract_mod = tf.keras.models.Model(ens_model.input, ens_model.layers[-5].output)\n",
    "            lstm_weight_array = [np.array(lstm_weight_extract_mod([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            lstm_weight_array = np.hstack(lstm_weight_array)\n",
    "            best_avg_lstm_weight = lstm_weight_array.mean(0).mean()            \n",
    "            print(\"cutting off thresh achieved\", flush = True)\n",
    "            # lstm_weight_extract_mod = tf.keras.models.Model(ens_model.input, ens_model.layers[-5].output)\n",
    "            # lstm_weight_array = [np.array(lstm_weight_extract_mod([train_valid_lstm, train_valid_doc2vec])) for i in range(0, bnn_reps)]\n",
    "            # lstm_weight_array = np.hstack(lstm_weight_array)\n",
    "            # cur_best_lstm_weight = lstm_weight_array.mean(0).mean()  \n",
    "            stop = datetime.now()\n",
    "            tt = stop-start\n",
    "            mins = tt.seconds/60.0            \n",
    "            return best_train_coverage, best_test_coverage, best_train_width, best_test_width, best_avg_lstm_weight,  exit_iter_thresh, \"cutoff_thresh_achieved\",mins\n",
    "        \n",
    "    print(\"something went awry\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb2ae5fe-5ecd-4b45-a492-883966cf79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch1 = second_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be0ee1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch1[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb492324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_train_logits_second[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cf12935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef051014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07cb7b8c-4634-46e7-ab75-e865c7116995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_lstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "107be4f2-7350-45cb-b00f-ce242ca16f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_idx =30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bca25306-fb43-479f-96f2-a3c55c984ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "uns_iter_threshold = 30\n",
    "threshold = 20\n",
    "cutoff_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c8965a0-55b0-4188-af15-1ea906d3e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_targets = 0.0001\n",
    "# rate = 0.5\n",
    "# bnn_reps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c319e243-394f-4dc8-8af3-db37ed80811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting off thresh not achieved\n"
     ]
    }
   ],
   "source": [
    "best_train_coverage, best_test_coverage, best_train_width, best_test_width, avg_lstm_weight, exit_iter, exit_status, mins_taken  = rep_one(cur_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a180a64d-57f5-4a8b-9353-f923f47f0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_coverage, best_test_coverage, avg_width = rep_one(cur_idx, second_lstm[cur_idx][0], second_lstm[cur_idx][1], second_lstm[cur_idx][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b347dd0-3d11-450a-846f-11672e0f9854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9242424242424242,\n",
       " 0.75,\n",
       " 0.2898831462984989,\n",
       " 0.272141184422071,\n",
       " 0.99853843,\n",
       " 13,\n",
       " 'cutoff_thresh_not_achieved',\n",
       " 1.1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_train_coverage, best_test_coverage, best_train_width, best_test_width, avg_lstm_weight, exit_iter, exit_status, mins_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "939e77e3-e29f-4d62-b141-10c2edda5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_train_coverage, best_test_coverage, best_train_width, best_test_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1322704-6836-43b6-9777-97501050b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_initial_targets_softmax.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8c79ef5-713a-4d91-baf5-686dad6fb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "# random_range = random.sample(range(0,best_initial_targets_test.shape[1]), k = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7318795a-840a-4e80-b305-9ed644491588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6112119-471c-4d3a-b506-cb2bc45b944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arviz as az\n",
    "# a = norm.rvs(size=10000)\n",
    "# # from hpd import hpd_grid\n",
    "# ci_95 = az.hdi(a, hdi_prob=0.95)\n",
    "# intervals, x, y, modes = hpd_grid(a, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37067474-4d72-4b78-a179-ee583b92d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expit(az.hdi(best_initial_targets_test[:, i, :], hdi_prob=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd88561e-e265-42a7-89e4-74cd0f80a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c6ab0ec-fd6b-4eda-bb06-e8763fb1eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(5, 4, figsize=(10, 10))\n",
    "# axes = axes.flatten()\n",
    "# counter = 0\n",
    "# for i in random_range: \n",
    "# #     plt.figure(figsize = (10, 10))\n",
    "#     axes[counter].hist(best_initial_targets_test[:, i, :])\n",
    "#     axes[counter].axvline(x= catch_test_logits[cur_idx][i], color = \"red\")\n",
    "#     u, l = np.percentile(best_initial_targets_test[:, i, :], q = (2.5, 97.5))\n",
    "#     axes[counter].axvline(x= u, color = \"green\")\n",
    "#     axes[counter].axvline(x= l, color = \"green\")    \n",
    "#     # ax[1].hist(soft_train[:, i, :][:,1])\n",
    "#     # ax[1].axvline(x= X_train_logits_true[i][1], color = \"red\")\n",
    "# #     ax[2].hist(soft_train[:, i, :][:,2])\n",
    "# #     ax[2].axvline(x= X_train_logits_true[i][2], color = \"red\")    \n",
    "#     counter += 1\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5ffc1f6-8bdb-4094-af42-515f3f7349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(5, 4, figsize=(10, 10))\n",
    "# axes = axes.flatten()\n",
    "# counter = 0\n",
    "# for i in random_range: \n",
    "# #     plt.figure(figsize = (10, 10))\n",
    "#     axes[counter].hist(best_initial_targets_softmax[:, i, :])\n",
    "#     axes[counter].axvline(x= catch_test_probs[cur_idx][i], color = \"red\")\n",
    "#     u, l = np.percentile(best_initial_targets_softmax[:, i, :], q = (2.5, 97.5))\n",
    "#     axes[counter].axvline(x= u, color = \"green\")\n",
    "#     axes[counter].axvline(x= l, color = \"green\")\n",
    "#     # ax[1].hist(soft_train[:, i, :][:,1])\n",
    "#     # ax[1].axvline(x= X_train_logits_true[i][1], color = \"red\")\n",
    "# #     ax[2].hist(soft_train[:, i, :][:,2])\n",
    "# #     ax[2].axvline(x= X_train_logits_true[i][2], color = \"red\")    \n",
    "#     counter += 1\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99a723c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done   2 tasks      | elapsed:   33.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "cutting off thresh not achieved\n",
      "cutting off thresh not achieved\n",
      "cutting off thresh not achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  11 tasks      | elapsed:   57.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "cutting off thresh not achieved\n",
      "cutting off thresh not achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "cutting off thresh not achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  20 tasks      | elapsed:  1.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "cutting off thresh not achieved\n",
      "thresh achieved\n",
      "cutting off thresh not achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  27 out of  50 | elapsed:  2.0min remaining:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting off thresh not achieved\n",
      "cutting off thresh not achieved\n",
      "cutting off thresh not achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  33 out of  50 | elapsed:  2.5min remaining:  1.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "cutting off thresh not achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  39 out of  50 | elapsed:  2.9min remaining:   48.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "cutting off thresh not achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  45 out of  50 | elapsed:  3.1min remaining:   20.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n",
      "thresh achieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=15)]: Done  50 out of  50 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "catch_coverages = Parallel(n_jobs=15, verbose = 10, backend = \"loky\")(delayed(rep_one)(i) for i in range(reps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e8200e5-b784-49c8-9ae9-fd7ee5a2e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "647980a7-9ae3-4c5c-b25c-2e37b6a84679",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.DataFrame(catch_coverages).iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa101efc-351b-4645-87a3-f008abf1ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77536655-0afd-4976-980a-fa7a28b55201",
   "metadata": {},
   "outputs": [],
   "source": [
    "check.columns = [\"train_coverage\", \"test_coverage\", \"avg_ci_width_train\", \"avg_ci_width_test\", \"avg_lstm_weight\", \"exit_iter\", \"exit_status\", \"time_taken\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fc1dcac-24b2-45e7-ae4c-e55eca90a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check.columns = [\"train_coverage\", \"test_coverage\", \"avg_ci_width_train\", \"avg_ci_width_test\", \"avg_lstm_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ead27b05-299e-49dc-bb97-236a18cd05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddb8e056-7721-487d-9764-eee11fce3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = check[[\"exit_status\"]].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95d71865-e7f8-4369-b057-db6f06ab0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.columns = [\"exit_status\", \"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1fe8c393-4728-443c-a448-a1c12da5cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.to_csv(\"exit_status_MCD_LSTM_Doc2Vec_\" + \"rate_\" + str(rate) + \"_bnn_reps_\" + str(bnn_reps) + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d22f4de-6617-402c-8e45-42f4fd668269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_451824/3676205243.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  check = check.drop([\"exit_status\"],1)\n"
     ]
    }
   ],
   "source": [
    "check = check.drop([\"exit_status\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48be9e74-4d3c-4848-a5eb-6830bc11dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mean = check.mean(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a209afc-6752-4449-8e7b-53790d143148",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mean.columns = [\"metrics\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "471094f4-a80d-4843-81c9-c7c3cafacae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_coverage</td>\n",
       "      <td>0.947576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_coverage</td>\n",
       "      <td>0.867500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg_ci_width_train</td>\n",
       "      <td>0.313201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avg_ci_width_test</td>\n",
       "      <td>0.324955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg_lstm_weight</td>\n",
       "      <td>0.736450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>exit_iter</td>\n",
       "      <td>17.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time_taken</td>\n",
       "      <td>0.915000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              metrics      value\n",
       "0      train_coverage   0.947576\n",
       "1       test_coverage   0.867500\n",
       "2  avg_ci_width_train   0.313201\n",
       "3   avg_ci_width_test   0.324955\n",
       "4     avg_lstm_weight   0.736450\n",
       "5           exit_iter  17.580000\n",
       "6          time_taken   0.915000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc1ce72a-636c-41af-8cb2-9b93acd820a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mean.to_csv(\"mean_metrics_MCD_LSTM_Doc2Vec_\" + \"rate\" + str(rate) + \"_bnn_reps_\" + str(bnn_reps) +  \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9076b54-b120-46de-b2d9-05ecc52f162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_median = check.median(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf8cee58-89eb-49d3-a311-a32579e29ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_median.columns = [\"metrics\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6f7dc47-478f-4d25-a494-eaf30a08eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_median = check.median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbb13ea5-bc7e-47ef-b0d2-ae90328699d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_median.to_csv(\"median_metrics_MCD_LSTM_Doc2Vec_\" + \"rate\" + str(rate) + \"_bnn_reps_\" + str(bnn_reps)+  \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4e9b60b-2887-430d-b77b-ad0fc442d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std = check.std(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40cac1af-b4a0-4a07-8204-b8bc9f0d8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std.columns = [\"metrics\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05eac583-9f15-43d6-b18c-e20a89d443b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_std.to_csv(\"std_dev_metrics_MCD_LSTM_Doc2Vec_\" + \"rate\" + str(rate) + \"_bnn_reps_\" + str(bnn_reps)+  \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f547c9a6-4eed-460e-ade6-8b0f962885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check.std(0)/np.sqrt(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0aed4c51-8860-4423-ad7b-d634bd950b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# # df = px.data.tips()\n",
    "# fig = px.histogram(check_melt, x=\"variable\", color=\"value\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "52816d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_one(0, second_lstm[0][0], second_lstm[0][1], second_lstm[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ab57b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_one(1, second_pred_train_logits[i], second_pred_valid_logits[i], second_pred_test_logits[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ec6b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep_one(3, second_pred_train_logits[i], second_pred_valid_logits[i], second_pred_test_logits[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "85e4fcba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# catch_all = []\n",
    "\n",
    "# for i in range(reps): \n",
    "#     catch_all.append(rep_one(i, second_pred_train_logits[i], second_pred_valid_logits[i], second_pred_test_logits[i])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1ad7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d94a8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(catch_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be901ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.std(catch_all)/np.sqrt(reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf69748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "162059fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_coverages = []\n",
    "# for i in tqdm(range(reps)): \n",
    "#     best_coverage, best_train_acc, best_valid_acc, best_test_acc = rep_one(i, second_lstm[i][0], second_lstm[i][1], second_lstm[i][2])\n",
    "#     print(best_coverage, best_train_acc, best_valid_acc, best_test_acc)\n",
    "#     catch_coverages.append([best_coverage, best_train_acc, best_valid_acc, best_test_acc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
